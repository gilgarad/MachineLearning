{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoderTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilgarad/mldl4automation/blob/master/code_tests/AutoEncoderTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jsD6YZ-U0m17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 참조: https://keraskorea.github.io/posts/2018-10-23-keras_autoencoder/\n",
        "# This is mostly copied from someone else, the purpose is to understand and test (could be changed a bit)"
      ]
    },
    {
      "metadata": {
        "id": "mgxHU6N40q_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "GQf_nsh3u6zt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.layers import LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.callbacks import TensorBoard\n",
        "import keras.backend.tensorflow_backend as K\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib 사용\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_J2r8p5l0tgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Data and set default variables"
      ]
    },
    {
      "metadata": {
        "id": "VMN1RaTF0xqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "2fbde9eb-bc4b-4b3b-9889-e2e5fb36567f"
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = 'drive/data/tb_logs' # Save in google drive path\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "\n",
        "get_ipython().system_raw(\n",
        "  'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "  .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#   \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-08 02:55:02--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.232.40.183, 52.73.9.93, 52.45.248.161, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.232.40.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  34.5MB/s    in 0.4s    \n",
            "\n",
            "2019-04-08 02:55:02 (34.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://3f75f667.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nR0gbCNLvLQy",
        "colab_type": "code",
        "outputId": "f9ecd229-dbf1-4a73-8c83-e531ad437854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print('Original x_train.shape:', x_train.shape)\n",
        "print('Original x_train.shape:', x_test.shape)\n",
        "      \n",
        "# Make all data to be between 0 to 1 \n",
        "x_train_flat = x_train / 255\n",
        "x_train_flat = x_train_flat.reshape(x_train_flat.shape[0], np.prod(x_train_flat.shape[1:]))\n",
        "x_test_flat = x_test / 255\n",
        "x_test_flat = x_test_flat.reshape(x_test_flat.shape[0], np.prod(x_test_flat.shape[1:]))\n",
        "\n",
        "print('Changed Shape (Flattened shape and normalize data as between 0 to 1)')\n",
        "print('Flattened x_train_flat.shape:', x_train_flat.shape)\n",
        "print('Flattened x_test_flat.shape:', x_test_flat.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original x_train.shape: (60000, 28, 28)\n",
            "Original x_train.shape: (10000, 28, 28)\n",
            "Changed Shape (Flattened shape and normalize data as between 0 to 1)\n",
            "Flattened x_train_flat.shape: (60000, 784)\n",
            "Flattened x_test_flat.shape: (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YeeN_9Jq3h21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tester Class for train and test"
      ]
    },
    {
      "metadata": {
        "id": "JmPo6VPMx10O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AutoEncoderTester:\n",
        "    def __init__(self, model):\n",
        "        \n",
        "        self.model = model\n",
        "       \n",
        "    def train(self, x_train, y_train, x_test, y_test, epochs=50, batch_size=256, \n",
        "              verbose=1):\n",
        "        \n",
        "        self.model.autoencoder.fit(x_train, y_train, \n",
        "                                   validation_data=(x_test, y_test), \n",
        "                                   epochs=epochs, batch_size=batch_size, \n",
        "                                   shuffle=True, \n",
        "                                   verbose=verbose,\n",
        "                                  callbacks=[\n",
        "                                      TensorBoard(log_dir=LOG_DIR, \n",
        "                                                  histogram_freq=1,\n",
        "                                                  write_graph=True,\n",
        "                                                  write_grads=True,\n",
        "#                                                   batch_size=batch_size,\n",
        "#                                                   write_images=True\n",
        "                                                 )\n",
        "                                  ])\n",
        "        \n",
        "    def test(self, x_test):\n",
        "        encoded_imgs = self.model.encoder.predict(x_test)\n",
        "        decoded_imgs = self.model.decoder.predict(encoded_imgs)\n",
        "        \n",
        "        n = 10  # 몇 개의 숫자를 나타낼 것인지\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        for i in range(n):\n",
        "            # 원본 데이터\n",
        "            ax = plt.subplot(2, n, i + 1)\n",
        "            plt.imshow(x_test[i].reshape(28, 28))\n",
        "            plt.gray()\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "\n",
        "            # 재구성된 데이터\n",
        "            ax = plt.subplot(2, n, i + 1 + n)\n",
        "            plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "            plt.gray()\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LcN75ORoIyi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "cW4qtlZd27Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        encode_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784, ))\n",
        "        encoded = Dense(encode_dim, activation='relu')(inputs)\n",
        "        decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "        autoencoder = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "\n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "js7vzr8tyFWG",
        "colab_type": "code",
        "outputId": "2b211240-a518-4fa1-bd6c-b20ec2c4d0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2573
        }
      },
      "cell_type": "code",
      "source": [
        "base_model = AutoEncoderTester(model=BaseModel())\n",
        "base_model.train(x_train=x_train_flat, y_train=x_train_flat, x_test=x_test_flat, y_test=x_test_flat,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "base_model.test(x_test=x_test_flat)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1144 - val_loss: 0.1126\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1142 - val_loss: 0.1124\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1139 - val_loss: 0.1121\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1137 - val_loss: 0.1119\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1134 - val_loss: 0.1116\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1132 - val_loss: 0.1114\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1130 - val_loss: 0.1112\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1127 - val_loss: 0.1109\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1125 - val_loss: 0.1107\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1123 - val_loss: 0.1105\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1121 - val_loss: 0.1103\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1119 - val_loss: 0.1101\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1117 - val_loss: 0.1099\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1115 - val_loss: 0.1097\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1113 - val_loss: 0.1095\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1111 - val_loss: 0.1093\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1109 - val_loss: 0.1091\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1107 - val_loss: 0.1089\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1105 - val_loss: 0.1087\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1103 - val_loss: 0.1085\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.1101 - val_loss: 0.1083\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1099 - val_loss: 0.1082\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1098 - val_loss: 0.1080\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1096 - val_loss: 0.1078\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1094 - val_loss: 0.1076\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1093 - val_loss: 0.1075\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1091 - val_loss: 0.1073\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1089 - val_loss: 0.1071\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1088 - val_loss: 0.1070\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1086 - val_loss: 0.1068\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1085 - val_loss: 0.1067\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1083 - val_loss: 0.1065\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1082 - val_loss: 0.1064\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1080 - val_loss: 0.1063\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1079 - val_loss: 0.1061\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1077 - val_loss: 0.1060\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.1076 - val_loss: 0.1058\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1075 - val_loss: 0.1057\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1073 - val_loss: 0.1056\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1072 - val_loss: 0.1054\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1071 - val_loss: 0.1053\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1069 - val_loss: 0.1052\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1068 - val_loss: 0.1051\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1067 - val_loss: 0.1049\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1066 - val_loss: 0.1048\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1065 - val_loss: 0.1047\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1063 - val_loss: 0.1046\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1062 - val_loss: 0.1045\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1061 - val_loss: 0.1044\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1060 - val_loss: 0.1043\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1059 - val_loss: 0.1042\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1058 - val_loss: 0.1040\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1057 - val_loss: 0.1039\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1056 - val_loss: 0.1038\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1055 - val_loss: 0.1037\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1054 - val_loss: 0.1036\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1053 - val_loss: 0.1035\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.1052 - val_loss: 0.1035\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.1051 - val_loss: 0.1034\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.1050 - val_loss: 0.1033\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1049 - val_loss: 0.1032\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1048 - val_loss: 0.1031\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1047 - val_loss: 0.1030\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1047 - val_loss: 0.1029\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1046 - val_loss: 0.1028\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1045 - val_loss: 0.1028\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1044 - val_loss: 0.1027\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1043 - val_loss: 0.1026\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.1042 - val_loss: 0.1025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-8NGZ-tT0N1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RegularizedModel:\n",
        "    def __init__(self):\n",
        "        encoding_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784,))\n",
        "        # L1 activity regularizer를 Dense layer에 추가 \n",
        "        encoded = Dense(encoding_dim, activation='relu',\n",
        "                        activity_regularizer=regularizers.l1(10e-5))(inputs)\n",
        "        decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "        decoder_layer = autoencoder.layers[-1]\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer(encoded_inputs))\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAhRNWF01RbS",
        "colab_type": "code",
        "outputId": "e85eca18-9cb2-452b-8b4f-3fa97e5b24af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7061
        }
      },
      "cell_type": "code",
      "source": [
        "regularized_model = AutoEncoderTester(RegularizedModel())\n",
        "regularized_model.train(x_train=x_train_flat, y_train=x_train_flat, x_test=x_test_flat, y_test=x_test_flat,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "regularized_model.test(x_test=x_test_flat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.7095 - val_loss: 0.6826\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.6765 - val_loss: 0.6704\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6648 - val_loss: 0.6591\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6538 - val_loss: 0.6483\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.6431 - val_loss: 0.6378\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.6328 - val_loss: 0.6277\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6229 - val_loss: 0.6180\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6134 - val_loss: 0.6087\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.6042 - val_loss: 0.5996\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5953 - val_loss: 0.5909\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5868 - val_loss: 0.5826\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5785 - val_loss: 0.5745\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5706 - val_loss: 0.5667\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5629 - val_loss: 0.5592\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5556 - val_loss: 0.5519\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5484 - val_loss: 0.5449\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5415 - val_loss: 0.5382\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5349 - val_loss: 0.5317\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5285 - val_loss: 0.5254\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.5223 - val_loss: 0.5193\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.5163 - val_loss: 0.5134\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.5105 - val_loss: 0.5077\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.5050 - val_loss: 0.5023\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4996 - val_loss: 0.4970\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4944 - val_loss: 0.4918\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4893 - val_loss: 0.4869\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4844 - val_loss: 0.4821\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4797 - val_loss: 0.4774\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4751 - val_loss: 0.4729\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4707 - val_loss: 0.4686\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.4664 - val_loss: 0.4644\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4623 - val_loss: 0.4603\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4583 - val_loss: 0.4563\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4544 - val_loss: 0.4525\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4506 - val_loss: 0.4488\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4469 - val_loss: 0.4452\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4434 - val_loss: 0.4417\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4399 - val_loss: 0.4383\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4366 - val_loss: 0.4350\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4333 - val_loss: 0.4318\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4302 - val_loss: 0.4287\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4271 - val_loss: 0.4257\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4241 - val_loss: 0.4228\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4213 - val_loss: 0.4199\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4184 - val_loss: 0.4172\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4157 - val_loss: 0.4145\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4131 - val_loss: 0.4119\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4105 - val_loss: 0.4093\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.4080 - val_loss: 0.4068\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4055 - val_loss: 0.4044\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4032 - val_loss: 0.4021\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4008 - val_loss: 0.3998\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3986 - val_loss: 0.3976\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3964 - val_loss: 0.3954\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3943 - val_loss: 0.3933\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3922 - val_loss: 0.3913\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3902 - val_loss: 0.3893\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3882 - val_loss: 0.3873\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3862 - val_loss: 0.3854\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3844 - val_loss: 0.3836\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3825 - val_loss: 0.3818\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3807 - val_loss: 0.3800\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3790 - val_loss: 0.3783\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3773 - val_loss: 0.3766\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3756 - val_loss: 0.3750\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.3740 - val_loss: 0.3734\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3724 - val_loss: 0.3718\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3709 - val_loss: 0.3703\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3694 - val_loss: 0.3688\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3679 - val_loss: 0.3673\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3665 - val_loss: 0.3659\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3650 - val_loss: 0.3645\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3637 - val_loss: 0.3631\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3623 - val_loss: 0.3618\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3610 - val_loss: 0.3605\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3597 - val_loss: 0.3592\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3585 - val_loss: 0.3580\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3572 - val_loss: 0.3568\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3560 - val_loss: 0.3556\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3548 - val_loss: 0.3544\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3537 - val_loss: 0.3533\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3525 - val_loss: 0.3521\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3514 - val_loss: 0.3510\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3504 - val_loss: 0.3500\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3493 - val_loss: 0.3489\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3482 - val_loss: 0.3479\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3472 - val_loss: 0.3469\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3462 - val_loss: 0.3459\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3452 - val_loss: 0.3449\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3443 - val_loss: 0.3440\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3433 - val_loss: 0.3430\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3424 - val_loss: 0.3421\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3415 - val_loss: 0.3412\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3406 - val_loss: 0.3403\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3398 - val_loss: 0.3395\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3389 - val_loss: 0.3386\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3381 - val_loss: 0.3378\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3372 - val_loss: 0.3370\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3364 - val_loss: 0.3362\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3356 - val_loss: 0.3354\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3349 - val_loss: 0.3346\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3341 - val_loss: 0.3339\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3334 - val_loss: 0.3331\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3326 - val_loss: 0.3324\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3319 - val_loss: 0.3317\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3312 - val_loss: 0.3310\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3305 - val_loss: 0.3303\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3298 - val_loss: 0.3296\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3291 - val_loss: 0.3289\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3285 - val_loss: 0.3283\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3278 - val_loss: 0.3276\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3272 - val_loss: 0.3270\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3265 - val_loss: 0.3264\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3259 - val_loss: 0.3258\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3253 - val_loss: 0.3252\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3247 - val_loss: 0.3246\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3241 - val_loss: 0.3240\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3236 - val_loss: 0.3234\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3230 - val_loss: 0.3228\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3224 - val_loss: 0.3223\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3219 - val_loss: 0.3217\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3213 - val_loss: 0.3212\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3208 - val_loss: 0.3207\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3203 - val_loss: 0.3202\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3198 - val_loss: 0.3196\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3192 - val_loss: 0.3191\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.3187 - val_loss: 0.3186\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3183 - val_loss: 0.3181\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3178 - val_loss: 0.3177\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3173 - val_loss: 0.3172\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3168 - val_loss: 0.3167\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3164 - val_loss: 0.3163\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3159 - val_loss: 0.3158\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3154 - val_loss: 0.3154\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3150 - val_loss: 0.3149\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3146 - val_loss: 0.3145\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3141 - val_loss: 0.3141\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3137 - val_loss: 0.3136\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3133 - val_loss: 0.3132\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3129 - val_loss: 0.3128\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3125 - val_loss: 0.3124\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.3121 - val_loss: 0.3120\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3117 - val_loss: 0.3116\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3113 - val_loss: 0.3112\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3109 - val_loss: 0.3108\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3105 - val_loss: 0.3105\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3102 - val_loss: 0.3101\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3098 - val_loss: 0.3097\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3094 - val_loss: 0.3094\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3091 - val_loss: 0.3090\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3087 - val_loss: 0.3087\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3084 - val_loss: 0.3083\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3080 - val_loss: 0.3080\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3077 - val_loss: 0.3076\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3073 - val_loss: 0.3073\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3070 - val_loss: 0.3070\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3067 - val_loss: 0.3067\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3064 - val_loss: 0.3063\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3061 - val_loss: 0.3060\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3057 - val_loss: 0.3057\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3054 - val_loss: 0.3054\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3051 - val_loss: 0.3051\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3048 - val_loss: 0.3048\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3045 - val_loss: 0.3045\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3042 - val_loss: 0.3042\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3039 - val_loss: 0.3039\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3037 - val_loss: 0.3036\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3034 - val_loss: 0.3033\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3031 - val_loss: 0.3031\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3028 - val_loss: 0.3028\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3025 - val_loss: 0.3025\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3023 - val_loss: 0.3022\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.3020 - val_loss: 0.3020\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.3017 - val_loss: 0.3017\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3015 - val_loss: 0.3015\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3012 - val_loss: 0.3012\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3010 - val_loss: 0.3009\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3007 - val_loss: 0.3007\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3005 - val_loss: 0.3004\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.3002 - val_loss: 0.3002\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.3000 - val_loss: 0.3000\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2997 - val_loss: 0.2997\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2995 - val_loss: 0.2995\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2993 - val_loss: 0.2993\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2990 - val_loss: 0.2990\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2988 - val_loss: 0.2988\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2986 - val_loss: 0.2986\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2984 - val_loss: 0.2983\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2981 - val_loss: 0.2981\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2979 - val_loss: 0.2979\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2977 - val_loss: 0.2977\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2975 - val_loss: 0.2975\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2973 - val_loss: 0.2973\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2971 - val_loss: 0.2971\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.2969 - val_loss: 0.2968\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2967 - val_loss: 0.2966\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2965 - val_loss: 0.2964\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2962 - val_loss: 0.2962\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2961 - val_loss: 0.2960\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.2959 - val_loss: 0.2958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Mmas2xDL1Uci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeepModel:\n",
        "    def __init__(self):\n",
        "        encode_dim = 32\n",
        "\n",
        "        inputs = Input(shape=(784,))\n",
        "        encoded = Dense(128, activation='relu')(inputs)\n",
        "        encoded = Dense(64, activation='relu')(encoded)\n",
        "        encoded = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "        decoded = Dense(64, activation='relu')(encoded)\n",
        "        decoded = Dense(128, activation='relu')(decoded)\n",
        "        decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        encoded_inputs = Input(shape=(encode_dim, ))\n",
        "#         decoder_layer = autoencoder.layers[-3:]\n",
        "        decoder_layer = encoded_inputs\n",
        "        for layer in autoencoder.layers[-3:]:\n",
        "            decoder_layer = layer(decoder_layer)\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer)\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zG70k1EL7eNV",
        "colab_type": "code",
        "outputId": "c9cb16aa-86c6-4d98-ecbc-fd0aeea9572e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7061
        }
      },
      "cell_type": "code",
      "source": [
        "deep_model = AutoEncoderTester(DeepModel())\n",
        "deep_model.train(x_train=x_train_flat, y_train=x_train_flat, x_test=x_test_flat, y_test=x_test_flat,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "deep_model.test(x_test=x_test_flat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.5071 - val_loss: 0.2734\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2699 - val_loss: 0.2668\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2659 - val_loss: 0.2649\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2637 - val_loss: 0.2622\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2610 - val_loss: 0.2597\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2581 - val_loss: 0.2570\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2545 - val_loss: 0.2518\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2501 - val_loss: 0.2472\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2459 - val_loss: 0.2429\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2417 - val_loss: 0.2390\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2379 - val_loss: 0.2351\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2320 - val_loss: 0.2275\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2246 - val_loss: 0.2201\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2185 - val_loss: 0.2152\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2151 - val_loss: 0.2131\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2128 - val_loss: 0.2106\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2102 - val_loss: 0.2074\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2078 - val_loss: 0.2061\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.2044 - val_loss: 0.2018\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.2013 - val_loss: 0.1989\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1981 - val_loss: 0.1957\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1958 - val_loss: 0.1931\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1933 - val_loss: 0.1919\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1917 - val_loss: 0.1897\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1898 - val_loss: 0.1879\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1884 - val_loss: 0.1862\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1866 - val_loss: 0.1851\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1847 - val_loss: 0.1820\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1829 - val_loss: 0.1806\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1814 - val_loss: 0.1789\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1795 - val_loss: 0.1771\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1780 - val_loss: 0.1753\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1763 - val_loss: 0.1743\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1748 - val_loss: 0.1761\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1735 - val_loss: 0.1703\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1718 - val_loss: 0.1697\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1709 - val_loss: 0.1687\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1696 - val_loss: 0.1672\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1684 - val_loss: 0.1662\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1671 - val_loss: 0.1654\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1659 - val_loss: 0.1646\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1655 - val_loss: 0.1623\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1639 - val_loss: 0.1622\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1628 - val_loss: 0.1612\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1620 - val_loss: 0.1601\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1604 - val_loss: 0.1594\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1599 - val_loss: 0.1574\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1586 - val_loss: 0.1566\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1579 - val_loss: 0.1557\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1564 - val_loss: 0.1555\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1561 - val_loss: 0.1535\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1551 - val_loss: 0.1527\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1543 - val_loss: 0.1521\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1529 - val_loss: 0.1510\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1529 - val_loss: 0.1504\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1522 - val_loss: 0.1510\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1515 - val_loss: 0.1498\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1503 - val_loss: 0.1490\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1501 - val_loss: 0.1482\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1495 - val_loss: 0.1469\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1486 - val_loss: 0.1487\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1483 - val_loss: 0.1457\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1476 - val_loss: 0.1453\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1468 - val_loss: 0.1456\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1462 - val_loss: 0.1447\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1461 - val_loss: 0.1432\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1453 - val_loss: 0.1454\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1446 - val_loss: 0.1438\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1446 - val_loss: 0.1432\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1441 - val_loss: 0.1419\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1435 - val_loss: 0.1416\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1426 - val_loss: 0.1415\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1425 - val_loss: 0.1407\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1418 - val_loss: 0.1404\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1411 - val_loss: 0.1413\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1408 - val_loss: 0.1400\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1407 - val_loss: 0.1387\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1396 - val_loss: 0.1385\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1393 - val_loss: 0.1375\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1390 - val_loss: 0.1376\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1385 - val_loss: 0.1358\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1381 - val_loss: 0.1361\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1371 - val_loss: 0.1352\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1374 - val_loss: 0.1352\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1368 - val_loss: 0.1347\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1364 - val_loss: 0.1351\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1359 - val_loss: 0.1338\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1356 - val_loss: 0.1339\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1352 - val_loss: 0.1334\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1349 - val_loss: 0.1345\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1346 - val_loss: 0.1326\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1340 - val_loss: 0.1330\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1336 - val_loss: 0.1321\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1336 - val_loss: 0.1319\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1331 - val_loss: 0.1303\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1328 - val_loss: 0.1306\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1323 - val_loss: 0.1312\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1321 - val_loss: 0.1306\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1315 - val_loss: 0.1294\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1310 - val_loss: 0.1288\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1312 - val_loss: 0.1287\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1307 - val_loss: 0.1297\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1306 - val_loss: 0.1280\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1300 - val_loss: 0.1274\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1296 - val_loss: 0.1288\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1296 - val_loss: 0.1287\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1292 - val_loss: 0.1278\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1286 - val_loss: 0.1269\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1287 - val_loss: 0.1273\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1280 - val_loss: 0.1263\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1284 - val_loss: 0.1272\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1274 - val_loss: 0.1276\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1275 - val_loss: 0.1258\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1270 - val_loss: 0.1247\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1268 - val_loss: 0.1241\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1266 - val_loss: 0.1243\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 1s 18us/step - loss: 0.1261 - val_loss: 0.1233\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1262 - val_loss: 0.1238\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1255 - val_loss: 0.1237\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1255 - val_loss: 0.1234\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1252 - val_loss: 0.1235\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1248 - val_loss: 0.1236\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1247 - val_loss: 0.1239\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1244 - val_loss: 0.1222\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1241 - val_loss: 0.1219\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1241 - val_loss: 0.1232\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1236 - val_loss: 0.1221\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1236 - val_loss: 0.1215\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1233 - val_loss: 0.1215\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1230 - val_loss: 0.1224\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1228 - val_loss: 0.1226\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1229 - val_loss: 0.1208\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1225 - val_loss: 0.1199\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1225 - val_loss: 0.1201\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1221 - val_loss: 0.1212\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1217 - val_loss: 0.1199\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1221 - val_loss: 0.1206\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1215 - val_loss: 0.1201\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1213 - val_loss: 0.1204\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1210 - val_loss: 0.1202\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1214 - val_loss: 0.1191\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1207 - val_loss: 0.1195\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1208 - val_loss: 0.1198\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1209 - val_loss: 0.1190\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1202 - val_loss: 0.1190\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1204 - val_loss: 0.1178\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1200 - val_loss: 0.1189\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1201 - val_loss: 0.1174\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1199 - val_loss: 0.1185\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1195 - val_loss: 0.1179\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1196 - val_loss: 0.1191\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1194 - val_loss: 0.1182\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1193 - val_loss: 0.1177\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1192 - val_loss: 0.1169\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1187 - val_loss: 0.1172\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1189 - val_loss: 0.1174\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1185 - val_loss: 0.1170\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1186 - val_loss: 0.1184\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1185 - val_loss: 0.1166\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1183 - val_loss: 0.1166\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1183 - val_loss: 0.1164\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1179 - val_loss: 0.1158\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1180 - val_loss: 0.1161\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1178 - val_loss: 0.1170\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1176 - val_loss: 0.1150\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1174 - val_loss: 0.1160\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1173 - val_loss: 0.1156\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1173 - val_loss: 0.1162\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1170 - val_loss: 0.1157\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1171 - val_loss: 0.1143\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1165 - val_loss: 0.1151\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1169 - val_loss: 0.1158\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1165 - val_loss: 0.1150\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1164 - val_loss: 0.1151\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1161 - val_loss: 0.1143\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1160 - val_loss: 0.1158\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1159 - val_loss: 0.1139\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1160 - val_loss: 0.1144\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1154 - val_loss: 0.1142\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1156 - val_loss: 0.1136\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1153 - val_loss: 0.1141\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1153 - val_loss: 0.1130\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1152 - val_loss: 0.1137\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1146 - val_loss: 0.1138\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1150 - val_loss: 0.1131\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1147 - val_loss: 0.1131\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1144 - val_loss: 0.1125\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1145 - val_loss: 0.1131\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1142 - val_loss: 0.1134\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1141 - val_loss: 0.1128\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1141 - val_loss: 0.1121\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1136 - val_loss: 0.1120\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1138 - val_loss: 0.1118\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1135 - val_loss: 0.1128\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1133 - val_loss: 0.1114\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1133 - val_loss: 0.1107\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.1129 - val_loss: 0.1127\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1131 - val_loss: 0.1121\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1131 - val_loss: 0.1111\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1129 - val_loss: 0.1109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAHKCBzUAAAAAAAA5wYMaAAAAAACAnPh/WlFm5vIgI2oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "D0roSg9z7ia3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvModel:\n",
        "    def __init__(self):\n",
        "\n",
        "        inputs = Input(shape=(28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "        x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "        # 이 시점에서 표현(representatoin)은 (4,4,8) 즉, 128 차원\n",
        "\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        # Decoder for Predict\n",
        "        encoded_inputs = Input(shape=(4, 4, 8))\n",
        "#         decoder_layer = autoencoder.layers[-3:]\n",
        "        decoder_layer = encoded_inputs\n",
        "        for layer in autoencoder.layers[-7:]:\n",
        "            decoder_layer = layer(decoder_layer)\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer)\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LclqdAUy8WXi",
        "colab_type": "code",
        "outputId": "d50446be-6393-4e06-96f5-c319061b2f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5921
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "# Reshape for conv2d\n",
        "x_train_3d = np.reshape(x_train_flat, (len(x_train_flat), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "x_test_3d = np.reshape(x_test_flat, (len(x_test_flat), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "conv_model = AutoEncoderTester(ConvModel())\n",
        "conv_model.train(x_train=x_train_3d, y_train=x_train_3d, x_test=x_test_3d, y_test=x_test_3d,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "conv_model.test(x_test=x_test_3d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://3f75f667.ngrok.io\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.3748 - val_loss: 0.2391\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.2247 - val_loss: 0.2164\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2061 - val_loss: 0.1971\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1935 - val_loss: 0.1913\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1851 - val_loss: 0.1814\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1776 - val_loss: 0.1769\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1718 - val_loss: 0.1697\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1667 - val_loss: 0.1610\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1640 - val_loss: 0.1617\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1601 - val_loss: 0.1601\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1569 - val_loss: 0.1531\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1544 - val_loss: 0.1506\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1518 - val_loss: 0.1478\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1497 - val_loss: 0.1476\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1469 - val_loss: 0.1438\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1454 - val_loss: 0.1429\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1437 - val_loss: 0.1409\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1415 - val_loss: 0.1400\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1405 - val_loss: 0.1398\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1391 - val_loss: 0.1399\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1378 - val_loss: 0.1365\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1366 - val_loss: 0.1333\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1357 - val_loss: 0.1336\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1337 - val_loss: 0.1333\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1344 - val_loss: 0.1291\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1327 - val_loss: 0.1326\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1316 - val_loss: 0.1303\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1312 - val_loss: 0.1300\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1307 - val_loss: 0.1289\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1300 - val_loss: 0.1268\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1290 - val_loss: 0.1277\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1279 - val_loss: 0.1250\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1278 - val_loss: 0.1271\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1272 - val_loss: 0.1255\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.1265 - val_loss: 0.1217\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1256 - val_loss: 0.1233\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1255 - val_loss: 0.1240\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1246 - val_loss: 0.1226\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1244 - val_loss: 0.1196\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1239 - val_loss: 0.1231\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1229 - val_loss: 0.1208\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1223 - val_loss: 0.1301\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1221 - val_loss: 0.1245\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1219 - val_loss: 0.1225\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1216 - val_loss: 0.1198\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1211 - val_loss: 0.1208\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1208 - val_loss: 0.1209\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1206 - val_loss: 0.1182\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1203 - val_loss: 0.1196\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1202 - val_loss: 0.1165\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1197 - val_loss: 0.1153\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1187 - val_loss: 0.1189\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1193 - val_loss: 0.1202\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1189 - val_loss: 0.1173\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1182 - val_loss: 0.1156\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1179 - val_loss: 0.1176\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1183 - val_loss: 0.1159\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1176 - val_loss: 0.1139\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1173 - val_loss: 0.1144\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1172 - val_loss: 0.1151\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1168 - val_loss: 0.1184\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1167 - val_loss: 0.1151\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1166 - val_loss: 0.1157\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1163 - val_loss: 0.1154\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1159 - val_loss: 0.1182\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1155 - val_loss: 0.1129\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1161 - val_loss: 0.1133\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1152 - val_loss: 0.1143\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1150 - val_loss: 0.1143\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1150 - val_loss: 0.1139\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1146 - val_loss: 0.1124\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1149 - val_loss: 0.1123\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.1141 - val_loss: 0.1152\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1144 - val_loss: 0.1150\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1139 - val_loss: 0.1116\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1138 - val_loss: 0.1116\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1134 - val_loss: 0.1107\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1134 - val_loss: 0.1104\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1130 - val_loss: 0.1111\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1125 - val_loss: 0.1131\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1131 - val_loss: 0.1117\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1124 - val_loss: 0.1104\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1123 - val_loss: 0.1110\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1121 - val_loss: 0.1112\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1118 - val_loss: 0.1104\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1123 - val_loss: 0.1117\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1119 - val_loss: 0.1099\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1116 - val_loss: 0.1110\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1117 - val_loss: 0.1119\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1116 - val_loss: 0.1100\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1108 - val_loss: 0.1073\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1112 - val_loss: 0.1125\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1108 - val_loss: 0.1073\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1106 - val_loss: 0.1103\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1109 - val_loss: 0.1094\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1101 - val_loss: 0.1097\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1104 - val_loss: 0.1098\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1105 - val_loss: 0.1076\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1101 - val_loss: 0.1107\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1100 - val_loss: 0.1073\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1097 - val_loss: 0.1093\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1093 - val_loss: 0.1113\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1096 - val_loss: 0.1094\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1094 - val_loss: 0.1077\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1096 - val_loss: 0.1089\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1091 - val_loss: 0.1076\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.1095 - val_loss: 0.1077\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.1090 - val_loss: 0.1086\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1090 - val_loss: 0.1053\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1087 - val_loss: 0.1073\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.1085 - val_loss: 0.1085\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1086 - val_loss: 0.1070\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1086 - val_loss: 0.1072\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1083 - val_loss: 0.1075\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.1079 - val_loss: 0.1110\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1083 - val_loss: 0.1076\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1082 - val_loss: 0.1055\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1077 - val_loss: 0.1067\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1083 - val_loss: 0.1056\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1076 - val_loss: 0.1059\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1076 - val_loss: 0.1056\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 0.1075 - val_loss: 0.1045\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1079 - val_loss: 0.1064\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1075 - val_loss: 0.1064\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.1072 - val_loss: 0.1074\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1073 - val_loss: 0.1039\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1070 - val_loss: 0.1059\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1068 - val_loss: 0.1059\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.1067 - val_loss: 0.1068\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1065 - val_loss: 0.1071\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.1068 - val_loss: 0.1069\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1067 - val_loss: 0.1046\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1065 - val_loss: 0.1055\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1059 - val_loss: 0.1062\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1064 - val_loss: 0.1061\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1059 - val_loss: 0.1067\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1063 - val_loss: 0.1049\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1058 - val_loss: 0.1049\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.1059 - val_loss: 0.1041\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1058 - val_loss: 0.1068\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1057 - val_loss: 0.1041\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1060 - val_loss: 0.1037\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 0.1057 - val_loss: 0.1060\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1057 - val_loss: 0.1046\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1056 - val_loss: 0.1065\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.1057 - val_loss: 0.1040\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1056 - val_loss: 0.1022\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1055 - val_loss: 0.1087\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1051 - val_loss: 0.1029\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1055 - val_loss: 0.1038\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.1054 - val_loss: 0.1060\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1051 - val_loss: 0.1057\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1050 - val_loss: 0.1066\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1051 - val_loss: 0.1042\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.1052 - val_loss: 0.1044\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.1050 - val_loss: 0.1015\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1049 - val_loss: 0.1029\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.1045 - val_loss: 0.1039\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1048 - val_loss: 0.1020\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.1045 - val_loss: 0.1036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0BAtZwJ56Dyv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Same as ConvModel, but number of filter increased\n",
        "class DenoiseConvModel:\n",
        "    def __init__(self):\n",
        "        \n",
        "        num_filters = 64\n",
        "        \n",
        "        inputs = Input(shape=(28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "        encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "        # 이 시점에서 표현(representatoin)은 (4,4,8) 즉, 128 차원\n",
        "\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(encoded)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Conv2D(num_filters, (3, 3), activation='relu')(x)\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        # Decoder for Predict\n",
        "        encoded_inputs = Input(shape=(4, 4, num_filters))\n",
        "#         decoder_layer = autoencoder.layers[-3:]\n",
        "        decoder_layer = encoded_inputs\n",
        "        for layer in autoencoder.layers[-7:]:\n",
        "            decoder_layer = layer(decoder_layer)\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer)\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXkaxrsO3x-r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape for conv2d\n",
        "x_train_3d = np.reshape(x_train_flat, (len(x_train_flat), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "x_test_3d = np.reshape(x_test_flat, (len(x_test_flat), 28, 28, 1))  # 'channels_firtst'이미지 데이터 형식을 사용하는 경우 이를 적용\n",
        "\n",
        "noise_factor = 0.5\n",
        "x_train_3d_noisy = x_train_3d + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_3d.shape) \n",
        "x_test_3d_noisy = x_test_3d + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_3d.shape) \n",
        "\n",
        "x_train_3d_noisy = np.clip(x_train_3d_noisy, 0., 1.)\n",
        "x_test_3d_noisy = np.clip(x_test_3d_noisy, 0., 1.)\n",
        "\n",
        "\n",
        "dconv_model = AutoEncoderTester(DenoiseConvModel())\n",
        "dconv_model.train(x_train=x_train_3d_noisy, y_train=x_train_3d, \n",
        "                 x_test=x_test_3d_noisy, y_test=x_test_3d,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "dconv_model.test(x_test=x_test_3d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DU7U2tzl7PMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Same as ConvModel, but number of filter increased\n",
        "class Seq2SeqModel:\n",
        "    def __init__(self):\n",
        "        \n",
        "        timesteps = 28\n",
        "        input_dim = 28\n",
        "        \n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "        decoded = RepeatVector(timesteps)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "        autoencoder = Model(inputs, decoded)\n",
        "        \n",
        "        encoder = Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "        # Decoder for Predict\n",
        "        encoded_inputs = Input(shape=(timesteps, input_dim))\n",
        "#         decoder_layer = autoencoder.layers[-3:]\n",
        "        decoder_layer = encoded_inputs\n",
        "        for layer in autoencoder.layers[-2:]:\n",
        "            decoder_layer = layer(decoder_layer)\n",
        "        decoder = Model(inputs=encoded_inputs, outputs=decoder_layer)\n",
        "\n",
        "\n",
        "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.autoencoder = autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AFwOaUSg4AQ6",
        "colab_type": "code",
        "outputId": "127839b3-1d36-4c42-d5cf-97f4eadc4f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dconv_model = AutoEncoderTester(DenoiseConvModel())\n",
        "dconv_model.train(x_train=x_train, y_train=x_train, \n",
        "                 x_test=x_test, y_test=x_test,\n",
        "                epochs=200, batch_size=1024, verbose=1)\n",
        "dconv_model.test(x_test=x_test_3d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.callbacks.TensorBoard"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "Kqq7HSJH4SZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
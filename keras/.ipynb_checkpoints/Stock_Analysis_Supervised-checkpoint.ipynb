{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주가 데이터를 일반적인 지도학습으로 학습합니다. Regression 모델과 Classification 모델을 시도해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.layers import Input, Dense, LSTM, GRU, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loader & Step estimator & train/test splitter\n",
    "## 링크: http://121.140.2.142:8888/notebooks/test_notes/preprocess/stockdata_preprocess_basic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(all_stock_data, batch_size, pre_data_length, pro_data_length, process_data):\n",
    "    num_company = len(all_stock_data)\n",
    "    num_features = all_stock_data[0].shape[1]\n",
    "    company_idx = list(range(num_company))\n",
    "    current_idx = np.array([0 for _ in range(num_company)])\n",
    "    \n",
    "    def get_batch(all_stock_data, current_idx, batch_company_idx, pre_data_length, pro_data_length):\n",
    "        batch_company = [all_stock_data[idx] for idx in batch_company_idx]\n",
    "        \n",
    "        # get cur pos of each data in mini batch\n",
    "        cur_pos = current_idx[batch_company_idx]\n",
    "        x_batch = np.empty(shape=(0, pre_data_length, num_features))\n",
    "        y_batch = np.empty(shape=(0, pro_data_length, num_features))\n",
    "        \n",
    "        for company, pos in zip(batch_company, cur_pos):\n",
    "            _x_batch = company[pos: pos+pre_data_length].reshape(1, pre_data_length, num_features)\n",
    "            _y_batch = company[pos+pre_data_length: pos+pre_data_length+pro_data_length].reshape(1, pro_data_length, num_features)\n",
    "            \n",
    "#             print(_new_x_batch)\n",
    "            x_batch = np.concatenate([x_batch, _x_batch], axis=0)\n",
    "            y_batch = np.concatenate([y_batch, _y_batch], axis=0)\n",
    "        \n",
    "        # update for next pos\n",
    "        new_pos = [c + 1 for c in cur_pos]\n",
    "        current_idx[batch_company_idx] = new_pos\n",
    "\n",
    "        # check if next pos possible, and remove from company_idx if not possible\n",
    "        for idx, (company, pos) in enumerate(zip(batch_company, new_pos)):\n",
    "            if company.shape[0] < pos + pre_data_length + pro_data_length:\n",
    "#                 print(company[0, 7], ' finished at pos:', pos)\n",
    "                company_idx.remove(batch_company_idx[idx])\n",
    "                np.delete(current_idx, batch_company_idx[idx])\n",
    "        \n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    count = 0\n",
    "    while True:\n",
    "        while len(company_idx) > 0:\n",
    "#             if count == 0:\n",
    "#                 print('New Epoch begins', len(company_idx))\n",
    "            count += 1\n",
    "            # random pick from all batch\n",
    "            if len(company_idx) >= batch_size:\n",
    "                batch_company_idx = sample(company_idx, batch_size)\n",
    "\n",
    "\n",
    "                x_batch, y_batch = get_batch(all_stock_data=all_stock_data, current_idx=current_idx, \n",
    "                                             pre_data_length=pre_data_length, pro_data_length=pro_data_length,\n",
    "                                             batch_company_idx=batch_company_idx)\n",
    "\n",
    "            else: # if number of company less than batch, then company should go for parallel\n",
    "                batch_company_idx = company_idx[:]\n",
    "                x_batch = list()\n",
    "                y_batch = list()\n",
    "                idx = 0\n",
    "                while len(x_batch) < batch_size and len(company_idx) > 0:\n",
    "                    if idx >= len(company_idx):\n",
    "                        idx = 0\n",
    "                    batch_company_idx = company_idx[idx]\n",
    "                    idx += 1\n",
    "                    idx = idx % len(company_idx)\n",
    "                    _x_batch, _y_batch = get_batch(all_stock_data=all_stock_data, current_idx=current_idx, \n",
    "                                                   pre_data_length=pre_data_length, pro_data_length=pro_data_length,\n",
    "                                                   batch_company_idx=[batch_company_idx])\n",
    "                    x_batch.append(_x_batch)\n",
    "                    y_batch.append(_y_batch)\n",
    "\n",
    "                x_batch = np.concatenate(x_batch, axis=0)\n",
    "                y_batch = np.concatenate(y_batch, axis=0)\n",
    "\n",
    "            # 마지막 처리를 해줍니다.\n",
    "            x_batch, y_batch = process_data(x_batch, y_batch)\n",
    "\n",
    "            yield x_batch, y_batch\n",
    "        \n",
    "        company_idx = list(range(num_company))\n",
    "        current_idx = np.array([0 for _ in range(num_company)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total estimated steps\n",
    "def get_estimated_steps(all_stock_data, pre_data_length, pro_data_length, batch_size):\n",
    "    \n",
    "    all_length = pre_data_length + pro_data_length\n",
    "\n",
    "    total_steps = 0\n",
    "    for stock_data in all_stock_data:\n",
    "        total_steps += (stock_data.shape[0] - all_length + 1)\n",
    "    #     print(total_steps)\n",
    "\n",
    "    return total_steps, int(np.ceil(total_steps / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_year(all_stock_data, year): # year 이전은 train, year 해당 년 및 이후는 test\n",
    "    train_stock_data = list()\n",
    "    test_stock_data = list()\n",
    "    for stock_data in all_stock_data:\n",
    "        indices = np.where(stock_data[:, 0] >= (year * 10000))[0]\n",
    "        test_stock_data.append(stock_data[indices][:, 1:])\n",
    "        \n",
    "        indices = np.where(stock_data[:, 0] < (year * 10000))[0]\n",
    "        train_stock_data.append(stock_data[indices][:, 1:])\n",
    "    \n",
    "    test_stock_data = [stock_data for stock_data in test_stock_data if stock_data.shape[0] > 0]\n",
    "    train_stock_data = [stock_data for stock_data in train_stock_data if stock_data.shape[0] > 0]\n",
    "    \n",
    "    return train_stock_data, test_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. '날짜': 'date', 2. '종가': 'final_price', 3. '전일비': 'compare_to_prior', 4. '시가': 'start_price',\n",
    "5. '고가': 'highest_price', 6. '저가': 'lowest_price', 7. '거래량': 'num_of_traded', 8. '회사이름': 'company_name'\n",
    "9. df_temp = Features.fnMACD(df_temp)\n",
    "    df_temp = Features.fnBolingerBand(df_temp)\n",
    "    df_temp = Features.fnRSI(df_temp)\n",
    "    df_temp = Features.fnStoch(df_temp)\n",
    "    df_temp = Features.change_prior_to(df_temp)\n",
    "    df_temp = Features.fnMA(df_temp, m_N=[5, 20, 60, 120, 240])\n",
    "\"\"\"\n",
    "\n",
    "def load_data(root_path):\n",
    "    all_stock_data = list()\n",
    "    for filename in listdir(root_path):\n",
    "        if not isfile(join(root_path, filename)) or '.npy' not in filename:\n",
    "            continue\n",
    "        print(filename)\n",
    "        stock_data = np.load(join(root_path, filename))\n",
    "        all_str_dates = stock_data[:, 0]\n",
    "        all_str_dates = [int(s.replace('.', '')) for s in all_str_dates]\n",
    "        stock_data[:, 0] = all_str_dates\n",
    "        stock_data = stock_data[::-1]\n",
    "        stock_data = stock_data[:, :-1] # remove and company name index\n",
    "        all_stock_data.append(stock_data)\n",
    "    \n",
    "    return all_stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and split train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/data1/stock_data_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경방.npy\n",
      "경보제약.npy\n",
      "경동도시가스.npy\n",
      "경동인베스트.npy\n"
     ]
    }
   ],
   "source": [
    "all_stock_data = load_data(root_path=root_path)\n",
    "train_stock_data, test_stock_data = split_train_test_by_year(all_stock_data=all_stock_data, year=2017) # this will also remove date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5172, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5172, 6)\n",
      "(374, 6)\n",
      "(4951, 6)\n"
     ]
    }
   ],
   "source": [
    "for stock_data in train_stock_data:\n",
    "    print(stock_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model train and test class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. t+1일의 가격으로 regression용 데이터와 classification용 데이터를 만들어주는 로직을 만들어 data_loader의 process_data로 넣을 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression example\n",
    "def process_regression_data(x_batch, y_batch):\n",
    "    \n",
    "    y_batch = y_batch[:, :, 0] # final price index = 1\n",
    "    \n",
    "    return x_batch, y_batch\n",
    "\n",
    "def process_classification_data(x_batch, y_batch):\n",
    "    \n",
    "    y_batch = y_batch[:, :, 2] # final price index = 1\n",
    "    y_batch[np.where(y_batch <= 0)[0], 0] = 0\n",
    "    y_batch[np.where(y_batch > 0)[0], 0] = 1\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 주어진 모델에 대해 학습하고 테스트 할 수 있는 클래스를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictionActivator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train(self, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, **kwargs):\n",
    "#         print(len(args))\n",
    "        \n",
    "        pre_data_length = kwargs['pre_data_length']\n",
    "        pro_data_length = kwargs['pro_data_length']\n",
    "        process_data = kwargs['process_data']\n",
    "        step_estimator = kwargs['step_estimator']\n",
    "        generator = kwargs['generator']\n",
    "        train_steps, train_steps_per_epoch = step_estimator(train_data, pre_data_length, pro_data_length, batch_size)\n",
    "        val_steps, val_steps_per_epoch = step_estimator(validation_data, pre_data_length, pro_data_length, batch_size)\n",
    "        \n",
    "        print('Trains steps:', train_steps_per_epoch, 'Test steps:', val_steps_per_epoch)\n",
    "        \n",
    "#         mc = ModelCheckpoint('./save/s2s_{epoch:03d}.h5', save_weights_only=True, period=5)\n",
    "        \n",
    "        self.model.fit_generator(generator=generator(all_stock_data=train_data, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       pre_data_length=pre_data_length, \n",
    "                                                       pro_data_length=pro_data_length,\n",
    "                                                       process_data=process_data),\n",
    "                                 steps_per_epoch=train_steps_per_epoch,\n",
    "                                 validation_data=generator(all_stock_data=validation_data, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       pre_data_length=pre_data_length, \n",
    "                                                       pro_data_length=pro_data_length,\n",
    "                                                       process_data=process_data),\n",
    "                                 validation_steps=val_steps_per_epoch,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=verbose,\n",
    "#                                  callbacks=[mc]\n",
    "                                )\n",
    "        print('')\n",
    "    \n",
    "    def test(self, test_data, tester, **kwargs):\n",
    "        pre_data_length = kwargs['pre_data_length']\n",
    "        pro_data_length = kwargs['pro_data_length']\n",
    "        process_data = kwargs['process_data']\n",
    "        step_estimator = kwargs['step_estimator']\n",
    "        \n",
    "        test_steps_per_epoch = step_estimator(test_data, pre_data_length, pro_data_length, batch_size)\n",
    "        tester(test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. lstm regression 심플 모델을 생성하고 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_lstm(pre_data_length, num_features, output_dim):\n",
    "    inputs = Input(shape=(pre_data_length, num_features))\n",
    "    lstm_layer = LSTM(64)\n",
    "    lstm_outputs = lstm_layer(inputs)\n",
    "#     flat_lstm_outputs = Flatten()(lstm_outputs)\n",
    "    outputs = Dense(output_dim)(lstm_outputs)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_length = 20\n",
    "pro_data_length = 1\n",
    "process_data = process_regression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 64)                18176     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 18,241\n",
      "Trainable params: 18,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Trains steps: 164 Test steps: 22\n",
      "Epoch 1/1000\n",
      "164/164 [==============================] - 4s 21ms/step - loss: 7127609037.3189 - acc: 0.0000e+00 - val_loss: 1294052659.7516 - val_acc: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7127268613.4388 - acc: 0.0000e+00 - val_loss: 1293919356.8678 - val_acc: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7126917940.3953 - acc: 0.0000e+00 - val_loss: 1293769010.3024 - val_acc: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7126529783.4193 - acc: 0.0000e+00 - val_loss: 1293610602.4018 - val_acc: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7126154748.5776 - acc: 0.0000e+00 - val_loss: 1293454515.8919 - val_acc: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7125781402.7420 - acc: 0.0000e+00 - val_loss: 1293301118.4573 - val_acc: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7125408390.2487 - acc: 0.0000e+00 - val_loss: 1293148154.4836 - val_acc: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7125035549.6493 - acc: 0.0000e+00 - val_loss: 1292996046.7261 - val_acc: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7124662728.8561 - acc: 0.0000e+00 - val_loss: 1292843910.2177 - val_acc: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7124290019.9686 - acc: 0.0000e+00 - val_loss: 1292691798.8137 - val_acc: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7123917137.6118 - acc: 0.0000e+00 - val_loss: 1292539630.0482 - val_acc: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "164/164 [==============================] - 2s 14ms/step - loss: 7123544413.6994 - acc: 0.0000e+00 - val_loss: 1292387549.2184 - val_acc: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7123171613.6857 - acc: 0.0000e+00 - val_loss: 1292235432.9993 - val_acc: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7122798905.5788 - acc: 0.0000e+00 - val_loss: 1292083400.0409 - val_acc: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7122426261.2778 - acc: 0.0000e+00 - val_loss: 1291931238.4280 - val_acc: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7122053527.2176 - acc: 0.0000e+00 - val_loss: 1291779249.6947 - val_acc: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7121680903.3070 - acc: 0.0000e+00 - val_loss: 1291627196.3068 - val_acc: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7121308241.4433 - acc: 0.0000e+00 - val_loss: 1291475172.3243 - val_acc: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7120935561.5319 - acc: 0.0000e+00 - val_loss: 1291323188.9204 - val_acc: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7120562962.8898 - acc: 0.0000e+00 - val_loss: 1291171075.1790 - val_acc: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7120190399.4680 - acc: 0.0000e+00 - val_loss: 1291019110.3813 - val_acc: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7119817728.5303 - acc: 0.0000e+00 - val_loss: 1290867125.8086 - val_acc: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7119445229.1118 - acc: 0.0000e+00 - val_loss: 1290715166.3404 - val_acc: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7119072624.3227 - acc: 0.0000e+00 - val_loss: 1290563228.4704 - val_acc: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7118700216.4184 - acc: 0.0000e+00 - val_loss: 1290411289.9459 - val_acc: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7118327668.6057 - acc: 0.0000e+00 - val_loss: 1290259276.1081 - val_acc: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7117955219.1390 - acc: 0.0000e+00 - val_loss: 1290107357.3587 - val_acc: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7117582687.0348 - acc: 0.0000e+00 - val_loss: 1289955460.7217 - val_acc: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7117210249.2747 - acc: 0.0000e+00 - val_loss: 1289803573.7151 - val_acc: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7116837834.1495 - acc: 0.0000e+00 - val_loss: 1289651685.3996 - val_acc: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7116465366.0464 - acc: 0.0000e+00 - val_loss: 1289499834.8108 - val_acc: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7116093085.0716 - acc: 0.0000e+00 - val_loss: 1289347960.2863 - val_acc: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7115720627.5054 - acc: 0.0000e+00 - val_loss: 1289196021.2476 - val_acc: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7115348293.0654 - acc: 0.0000e+00 - val_loss: 1289044172.4821 - val_acc: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7114975923.0139 - acc: 0.0000e+00 - val_loss: 1288892360.7889 - val_acc: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7114603644.6728 - acc: 0.0000e+00 - val_loss: 1288740541.2885 - val_acc: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7114231307.1096 - acc: 0.0000e+00 - val_loss: 1288588773.3061 - val_acc: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7113859050.9151 - acc: 0.0000e+00 - val_loss: 1288436944.8298 - val_acc: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7113486754.5240 - acc: 0.0000e+00 - val_loss: 1288285161.2330 - val_acc: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7113114516.5735 - acc: 0.0000e+00 - val_loss: 1288133435.6523 - val_acc: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7112742409.2621 - acc: 0.0000e+00 - val_loss: 1287981646.7261 - val_acc: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7112370095.4065 - acc: 0.0000e+00 - val_loss: 1287829902.0248 - val_acc: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7111997936.7752 - acc: 0.0000e+00 - val_loss: 1287678157.3236 - val_acc: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7111625662.5296 - acc: 0.0000e+00 - val_loss: 1287526464.6545 - val_acc: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7111253579.9004 - acc: 0.0000e+00 - val_loss: 1287374651.7925 - val_acc: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7110881436.5872 - acc: 0.0000e+00 - val_loss: 1287222957.9547 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7110509351.2261 - acc: 0.0000e+00 - val_loss: 1287071256.3097 - val_acc: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7110137187.6185 - acc: 0.0000e+00 - val_loss: 1286919611.5121 - val_acc: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7109765200.0158 - acc: 0.0000e+00 - val_loss: 1286767924.8269 - val_acc: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7109393130.9474 - acc: 0.0000e+00 - val_loss: 1286616286.0131 - val_acc: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7109021154.2719 - acc: 0.0000e+00 - val_loss: 1286464621.4405 - val_acc: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7108649011.3468 - acc: 0.0000e+00 - val_loss: 1286312997.5866 - val_acc: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7108277093.6983 - acc: 0.0000e+00 - val_loss: 1286161374.9014 - val_acc: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7107905111.4611 - acc: 0.0000e+00 - val_loss: 1286009777.3207 - val_acc: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7107533112.3447 - acc: 0.0000e+00 - val_loss: 1285858152.2980 - val_acc: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7107161079.1789 - acc: 0.0000e+00 - val_loss: 1285706595.4361 - val_acc: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7106789188.6521 - acc: 0.0000e+00 - val_loss: 1285554993.0402 - val_acc: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7106417295.0022 - acc: 0.0000e+00 - val_loss: 1285403452.3068 - val_acc: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7106045425.7434 - acc: 0.0000e+00 - val_loss: 1285251891.2841 - val_acc: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7105673519.2161 - acc: 0.0000e+00 - val_loss: 1285100354.7115 - val_acc: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7105301664.6889 - acc: 0.0000e+00 - val_loss: 1284948797.1950 - val_acc: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7104929808.8939 - acc: 0.0000e+00 - val_loss: 1284797312.1402 - val_acc: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7104557977.9773 - acc: 0.0000e+00 - val_loss: 1284645755.7925 - val_acc: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7104186160.6216 - acc: 0.0000e+00 - val_loss: 1284494290.5128 - val_acc: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7103814452.4409 - acc: 0.0000e+00 - val_loss: 1284342882.4543 - val_acc: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7103442611.4746 - acc: 0.0000e+00 - val_loss: 1284191430.3112 - val_acc: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7103070959.0996 - acc: 0.0000e+00 - val_loss: 1284039945.7706 - val_acc: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7102699181.0629 - acc: 0.0000e+00 - val_loss: 1283888518.7319 - val_acc: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7102327583.5190 - acc: 0.0000e+00 - val_loss: 1283737056.9584 - val_acc: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7101955763.7237 - acc: 0.0000e+00 - val_loss: 1283585623.2812 - val_acc: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7101584111.5438 - acc: 0.0000e+00 - val_loss: 1283434207.5559 - val_acc: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7101212511.7543 - acc: 0.0000e+00 - val_loss: 1283282789.4931 - val_acc: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7100840833.6223 - acc: 0.0000e+00 - val_loss: 1283131441.9284 - val_acc: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7100469232.5651 - acc: 0.0000e+00 - val_loss: 1282980028.0263 - val_acc: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7100097649.3626 - acc: 0.0000e+00 - val_loss: 1282828662.5099 - val_acc: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7099726124.7947 - acc: 0.0000e+00 - val_loss: 1282677392.7363 - val_acc: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7099354524.6161 - acc: 0.0000e+00 - val_loss: 1282526036.8503 - val_acc: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7098982994.6815 - acc: 0.0000e+00 - val_loss: 1282374701.9080 - val_acc: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7098611446.4060 - acc: 0.0000e+00 - val_loss: 1282223384.2630 - val_acc: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7098239953.7411 - acc: 0.0000e+00 - val_loss: 1282072089.3850 - val_acc: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7097868479.2242 - acc: 0.0000e+00 - val_loss: 1281920748.4587 - val_acc: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7097496956.5094 - acc: 0.0000e+00 - val_loss: 1281769476.8619 - val_acc: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7097125625.0207 - acc: 0.0000e+00 - val_loss: 1281618289.0402 - val_acc: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7096754105.4280 - acc: 0.0000e+00 - val_loss: 1281466996.4996 - val_acc: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7096382757.6455 - acc: 0.0000e+00 - val_loss: 1281315758.4690 - val_acc: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7096011370.8373 - acc: 0.0000e+00 - val_loss: 1281164527.5909 - val_acc: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7095639927.5399 - acc: 0.0000e+00 - val_loss: 1281013276.9379 - val_acc: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7095268625.0262 - acc: 0.0000e+00 - val_loss: 1280862055.0358 - val_acc: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7094897270.1209 - acc: 0.0000e+00 - val_loss: 1280710914.4310 - val_acc: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7094525983.9980 - acc: 0.0000e+00 - val_loss: 1280559699.6815 - val_acc: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7094154632.8983 - acc: 0.0000e+00 - val_loss: 1280408516.6749 - val_acc: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7093783394.5808 - acc: 0.0000e+00 - val_loss: 1280257358.7728 - val_acc: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7093412138.1174 - acc: 0.0000e+00 - val_loss: 1280106152.9993 - val_acc: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7093040866.6292 - acc: 0.0000e+00 - val_loss: 1279955108.1373 - val_acc: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7092669689.6801 - acc: 0.0000e+00 - val_loss: 1279803942.4280 - val_acc: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7092298461.8015 - acc: 0.0000e+00 - val_loss: 1279652827.5822 - val_acc: 0.0000e+00\n",
      "Epoch 97/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 7091927217.8263 - acc: 0.0000e+00 - val_loss: 1279501721.7122 - val_acc: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7091556141.4656 - acc: 0.0000e+00 - val_loss: 1279350672.0351 - val_acc: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7091184869.8770 - acc: 0.0000e+00 - val_loss: 1279199587.1088 - val_acc: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7090813841.4209 - acc: 0.0000e+00 - val_loss: 1279048509.3353 - val_acc: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7090442717.1544 - acc: 0.0000e+00 - val_loss: 1278897392.6662 - val_acc: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7090071549.3753 - acc: 0.0000e+00 - val_loss: 1278746450.6998 - val_acc: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7089700520.6249 - acc: 0.0000e+00 - val_loss: 1278595372.9262 - val_acc: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7089329502.8016 - acc: 0.0000e+00 - val_loss: 1278444325.0723 - val_acc: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7088958444.8789 - acc: 0.0000e+00 - val_loss: 1278293304.1461 - val_acc: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7088587482.7643 - acc: 0.0000e+00 - val_loss: 1278142342.4047 - val_acc: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7088216422.2068 - acc: 0.0000e+00 - val_loss: 1277991378.3258 - val_acc: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7087845446.2370 - acc: 0.0000e+00 - val_loss: 1277840386.6647 - val_acc: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7087474522.1725 - acc: 0.0000e+00 - val_loss: 1277689469.8028 - val_acc: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7087103596.2537 - acc: 0.0000e+00 - val_loss: 1277538506.8926 - val_acc: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7086732642.0402 - acc: 0.0000e+00 - val_loss: 1277387581.0548 - val_acc: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7086361805.6855 - acc: 0.0000e+00 - val_loss: 1277236678.4982 - val_acc: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7085990815.9586 - acc: 0.0000e+00 - val_loss: 1277085747.3309 - val_acc: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7085619949.9421 - acc: 0.0000e+00 - val_loss: 1276934836.4529 - val_acc: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7085249200.4185 - acc: 0.0000e+00 - val_loss: 1276783972.7918 - val_acc: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7084878410.6985 - acc: 0.0000e+00 - val_loss: 1276633078.0424 - val_acc: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7084507459.4108 - acc: 0.0000e+00 - val_loss: 1276482180.9554 - val_acc: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7084136748.6197 - acc: 0.0000e+00 - val_loss: 1276331360.3506 - val_acc: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7083765952.2642 - acc: 0.0000e+00 - val_loss: 1276180480.5610 - val_acc: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7083395172.5916 - acc: 0.0000e+00 - val_loss: 1276029758.6910 - val_acc: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7083024507.3625 - acc: 0.0000e+00 - val_loss: 1275878873.5720 - val_acc: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7082653785.0577 - acc: 0.0000e+00 - val_loss: 1275728051.7984 - val_acc: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7082283095.9248 - acc: 0.0000e+00 - val_loss: 1275577341.8963 - val_acc: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7081912407.7676 - acc: 0.0000e+00 - val_loss: 1275426502.8254 - val_acc: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7081541803.5145 - acc: 0.0000e+00 - val_loss: 1275275765.3411 - val_acc: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7081171016.8162 - acc: 0.0000e+00 - val_loss: 1275125000.4149 - val_acc: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "164/164 [==============================] - 2s 14ms/step - loss: 7080800427.1978 - acc: 0.0000e+00 - val_loss: 1274974206.2235 - val_acc: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7080429884.0211 - acc: 0.0000e+00 - val_loss: 1274823529.7473 - val_acc: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7080059319.8665 - acc: 0.0000e+00 - val_loss: 1274672764.8210 - val_acc: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7079688736.1019 - acc: 0.0000e+00 - val_loss: 1274522135.7020 - val_acc: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7079318198.6799 - acc: 0.0000e+00 - val_loss: 1274371389.8963 - val_acc: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7078947663.5013 - acc: 0.0000e+00 - val_loss: 1274220738.0102 - val_acc: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7078577109.2971 - acc: 0.0000e+00 - val_loss: 1274070038.9072 - val_acc: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7078206635.8773 - acc: 0.0000e+00 - val_loss: 1273919334.3346 - val_acc: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7077836125.5770 - acc: 0.0000e+00 - val_loss: 1273768691.4244 - val_acc: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7077465776.4530 - acc: 0.0000e+00 - val_loss: 1273618016.2571 - val_acc: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7077095327.8134 - acc: 0.0000e+00 - val_loss: 1273467459.4595 - val_acc: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7076724922.4924 - acc: 0.0000e+00 - val_loss: 1273316818.3725 - val_acc: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7076354553.4642 - acc: 0.0000e+00 - val_loss: 1273166217.8641 - val_acc: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7075984144.1425 - acc: 0.0000e+00 - val_loss: 1273015598.8897 - val_acc: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7075613697.6482 - acc: 0.0000e+00 - val_loss: 1272865086.9715 - val_acc: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7075243384.6219 - acc: 0.0000e+00 - val_loss: 1272714478.7962 - val_acc: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7074873110.1329 - acc: 0.0000e+00 - val_loss: 1272563985.9985 - val_acc: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7074502891.4519 - acc: 0.0000e+00 - val_loss: 1272413475.2491 - val_acc: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7074132609.0597 - acc: 0.0000e+00 - val_loss: 1272262892.1782 - val_acc: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7073762253.2991 - acc: 0.0000e+00 - val_loss: 1272112423.3163 - val_acc: 0.0000e+00\n",
      "Epoch 147/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 7073392093.2526 - acc: 0.0000e+00 - val_loss: 1271961874.3258 - val_acc: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7073021881.3019 - acc: 0.0000e+00 - val_loss: 1271811391.6728 - val_acc: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7072651733.6454 - acc: 0.0000e+00 - val_loss: 1271660884.5698 - val_acc: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7072281467.7422 - acc: 0.0000e+00 - val_loss: 1271510463.5793 - val_acc: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7071911268.5707 - acc: 0.0000e+00 - val_loss: 1271360027.6289 - val_acc: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7071541087.3521 - acc: 0.0000e+00 - val_loss: 1271209530.6706 - val_acc: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7071171067.4069 - acc: 0.0000e+00 - val_loss: 1271059122.8167 - val_acc: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7070800995.9476 - acc: 0.0000e+00 - val_loss: 1270908650.9627 - val_acc: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7070430833.0701 - acc: 0.0000e+00 - val_loss: 1270758296.9642 - val_acc: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7070060724.7304 - acc: 0.0000e+00 - val_loss: 1270607846.0541 - val_acc: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7069690664.0022 - acc: 0.0000e+00 - val_loss: 1270457511.1760 - val_acc: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7069320744.7433 - acc: 0.0000e+00 - val_loss: 1270307132.0730 - val_acc: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7068950630.1584 - acc: 0.0000e+00 - val_loss: 1270156705.0986 - val_acc: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7068580523.9652 - acc: 0.0000e+00 - val_loss: 1270006415.1001 - val_acc: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7068210615.2422 - acc: 0.0000e+00 - val_loss: 1269856124.4470 - val_acc: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7067840661.3463 - acc: 0.0000e+00 - val_loss: 1269705718.4164 - val_acc: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7067470745.7926 - acc: 0.0000e+00 - val_loss: 1269555403.8276 - val_acc: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7067100760.1878 - acc: 0.0000e+00 - val_loss: 1269405055.2988 - val_acc: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7066730921.0276 - acc: 0.0000e+00 - val_loss: 1269254800.5493 - val_acc: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7066360907.2261 - acc: 0.0000e+00 - val_loss: 1269104571.5588 - val_acc: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7065991081.4316 - acc: 0.0000e+00 - val_loss: 1268954224.7129 - val_acc: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7065621286.9537 - acc: 0.0000e+00 - val_loss: 1268803968.7947 - val_acc: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7065251391.0099 - acc: 0.0000e+00 - val_loss: 1268653750.6034 - val_acc: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7064881600.2394 - acc: 0.0000e+00 - val_loss: 1268503452.7977 - val_acc: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7064511711.9045 - acc: 0.0000e+00 - val_loss: 1268353280.0000 - val_acc: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7064141959.3803 - acc: 0.0000e+00 - val_loss: 1268203109.6801 - val_acc: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7063772182.1707 - acc: 0.0000e+00 - val_loss: 1268052838.8020 - val_acc: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7063402458.3304 - acc: 0.0000e+00 - val_loss: 1267902677.9722 - val_acc: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7063032726.4876 - acc: 0.0000e+00 - val_loss: 1267752535.0942 - val_acc: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7062663082.3557 - acc: 0.0000e+00 - val_loss: 1267602297.1278 - val_acc: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7062293377.8309 - acc: 0.0000e+00 - val_loss: 1267452130.9686 - val_acc: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7061923696.6235 - acc: 0.0000e+00 - val_loss: 1267302065.2272 - val_acc: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7061553996.0983 - acc: 0.0000e+00 - val_loss: 1267151967.2286 - val_acc: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7061184386.3090 - acc: 0.0000e+00 - val_loss: 1267001781.9489 - val_acc: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7060814796.4218 - acc: 0.0000e+00 - val_loss: 1266851686.9423 - val_acc: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7060445241.6577 - acc: 0.0000e+00 - val_loss: 1266701591.9357 - val_acc: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7060075665.6235 - acc: 0.0000e+00 - val_loss: 1266551468.1782 - val_acc: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7059706089.7855 - acc: 0.0000e+00 - val_loss: 1266401415.0592 - val_acc: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7059336453.7498 - acc: 0.0000e+00 - val_loss: 1266251396.0205 - val_acc: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7058966940.9373 - acc: 0.0000e+00 - val_loss: 1266101350.0541 - val_acc: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7058597380.8066 - acc: 0.0000e+00 - val_loss: 1265951324.3769 - val_acc: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7058227961.6547 - acc: 0.0000e+00 - val_loss: 1265801243.1614 - val_acc: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7057858484.0635 - acc: 0.0000e+00 - val_loss: 1265651274.9861 - val_acc: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7057489103.2538 - acc: 0.0000e+00 - val_loss: 1265501249.3090 - val_acc: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7057119676.1998 - acc: 0.0000e+00 - val_loss: 1265351330.1738 - val_acc: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7056750235.8770 - acc: 0.0000e+00 - val_loss: 1265201260.9262 - val_acc: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7056380844.7255 - acc: 0.0000e+00 - val_loss: 1265051323.1848 - val_acc: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7056011497.8683 - acc: 0.0000e+00 - val_loss: 1264901396.2425 - val_acc: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7055642158.8161 - acc: 0.0000e+00 - val_loss: 1264751471.1234 - val_acc: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7055272781.5188 - acc: 0.0000e+00 - val_loss: 1264601556.1490 - val_acc: 0.0000e+00\n",
      "Epoch 197/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 7054903434.3694 - acc: 0.0000e+00 - val_loss: 1264451644.1665 - val_acc: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7054534165.3674 - acc: 0.0000e+00 - val_loss: 1264301672.9993 - val_acc: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7054164865.6320 - acc: 0.0000e+00 - val_loss: 1264151801.0811 - val_acc: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7053795603.2654 - acc: 0.0000e+00 - val_loss: 1264001936.9701 - val_acc: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7053426368.8982 - acc: 0.0000e+00 - val_loss: 1263852045.2768 - val_acc: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7053057163.9954 - acc: 0.0000e+00 - val_loss: 1263702212.9087 - val_acc: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7052687971.3867 - acc: 0.0000e+00 - val_loss: 1263552394.8459 - val_acc: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7052318806.0939 - acc: 0.0000e+00 - val_loss: 1263402448.7831 - val_acc: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7051949639.8266 - acc: 0.0000e+00 - val_loss: 1263252640.8649 - val_acc: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7051580431.3133 - acc: 0.0000e+00 - val_loss: 1263102871.8422 - val_acc: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7051211220.1664 - acc: 0.0000e+00 - val_loss: 1262953066.9160 - val_acc: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7050842071.2644 - acc: 0.0000e+00 - val_loss: 1262803290.0862 - val_acc: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7050473031.1471 - acc: 0.0000e+00 - val_loss: 1262653542.0073 - val_acc: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7050103998.9320 - acc: 0.0000e+00 - val_loss: 1262503798.7436 - val_acc: 0.0000e+00\n",
      "Epoch 211/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7049734877.2499 - acc: 0.0000e+00 - val_loss: 1262354044.6808 - val_acc: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7049365858.5957 - acc: 0.0000e+00 - val_loss: 1262204321.7064 - val_acc: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7048996831.0627 - acc: 0.0000e+00 - val_loss: 1262054595.7400 - val_acc: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7048627878.9473 - acc: 0.0000e+00 - val_loss: 1261904812.4120 - val_acc: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7048258813.8514 - acc: 0.0000e+00 - val_loss: 1261755105.5661 - val_acc: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7047889986.1301 - acc: 0.0000e+00 - val_loss: 1261605435.2783 - val_acc: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7047521025.8182 - acc: 0.0000e+00 - val_loss: 1261455773.3119 - val_acc: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7047152095.5562 - acc: 0.0000e+00 - val_loss: 1261306101.2009 - val_acc: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7046783154.9524 - acc: 0.0000e+00 - val_loss: 1261156445.2184 - val_acc: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "164/164 [==============================] - 2s 14ms/step - loss: 7046414299.1314 - acc: 0.0000e+00 - val_loss: 1261006803.0270 - val_acc: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7046045499.9947 - acc: 0.0000e+00 - val_loss: 1260857187.1088 - val_acc: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7045676622.1242 - acc: 0.0000e+00 - val_loss: 1260707547.2549 - val_acc: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7045307772.1571 - acc: 0.0000e+00 - val_loss: 1260557970.8868 - val_acc: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7044938944.4335 - acc: 0.0000e+00 - val_loss: 1260408372.9204 - val_acc: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7044570116.9051 - acc: 0.0000e+00 - val_loss: 1260258784.5844 - val_acc: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7044201418.3567 - acc: 0.0000e+00 - val_loss: 1260109226.6822 - val_acc: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7043832655.6110 - acc: 0.0000e+00 - val_loss: 1259959632.8766 - val_acc: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7043463954.1346 - acc: 0.0000e+00 - val_loss: 1259810085.7736 - val_acc: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7043095185.5339 - acc: 0.0000e+00 - val_loss: 1259660559.6143 - val_acc: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7042726472.5459 - acc: 0.0000e+00 - val_loss: 1259511012.5113 - val_acc: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7042357791.8513 - acc: 0.0000e+00 - val_loss: 1259361511.4565 - val_acc: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7041989183.6463 - acc: 0.0000e+00 - val_loss: 1259211989.4580 - val_acc: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7041620532.4150 - acc: 0.0000e+00 - val_loss: 1259062532.1140 - val_acc: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7041251824.4012 - acc: 0.0000e+00 - val_loss: 1258913041.2038 - val_acc: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "164/164 [==============================] - 2s 14ms/step - loss: 7040883243.8064 - acc: 0.0000e+00 - val_loss: 1258763567.0767 - val_acc: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7040514685.7484 - acc: 0.0000e+00 - val_loss: 1258614089.9576 - val_acc: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7040146118.6175 - acc: 0.0000e+00 - val_loss: 1258464731.2082 - val_acc: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7039777548.3646 - acc: 0.0000e+00 - val_loss: 1258315282.8400 - val_acc: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7039409049.1373 - acc: 0.0000e+00 - val_loss: 1258165863.7370 - val_acc: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7039040513.8113 - acc: 0.0000e+00 - val_loss: 1258016453.6099 - val_acc: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7038671997.5097 - acc: 0.0000e+00 - val_loss: 1257867035.6757 - val_acc: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7038303575.9437 - acc: 0.0000e+00 - val_loss: 1257717652.4763 - val_acc: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7037935112.9123 - acc: 0.0000e+00 - val_loss: 1257568254.3170 - val_acc: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7037566616.4157 - acc: 0.0000e+00 - val_loss: 1257418892.0614 - val_acc: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7037198202.1665 - acc: 0.0000e+00 - val_loss: 1257269511.1994 - val_acc: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7036829841.5765 - acc: 0.0000e+00 - val_loss: 1257120181.8554 - val_acc: 0.0000e+00\n",
      "Epoch 247/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 7036461471.2311 - acc: 0.0000e+00 - val_loss: 1256970848.8649 - val_acc: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7036093169.7651 - acc: 0.0000e+00 - val_loss: 1256821612.7860 - val_acc: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7035724732.0993 - acc: 0.0000e+00 - val_loss: 1256672286.9481 - val_acc: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7035356375.4119 - acc: 0.0000e+00 - val_loss: 1256522975.5559 - val_acc: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7034988107.1178 - acc: 0.0000e+00 - val_loss: 1256373711.3806 - val_acc: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7034619796.4796 - acc: 0.0000e+00 - val_loss: 1256224422.1008 - val_acc: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7034251542.0385 - acc: 0.0000e+00 - val_loss: 1256075165.0782 - val_acc: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7033883260.9629 - acc: 0.0000e+00 - val_loss: 1255925917.0314 - val_acc: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7033515005.1559 - acc: 0.0000e+00 - val_loss: 1255776738.9686 - val_acc: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7033146786.1301 - acc: 0.0000e+00 - val_loss: 1255627516.6808 - val_acc: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7032778609.6423 - acc: 0.0000e+00 - val_loss: 1255478304.5376 - val_acc: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7032410438.0327 - acc: 0.0000e+00 - val_loss: 1255329068.4587 - val_acc: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7032042311.3998 - acc: 0.0000e+00 - val_loss: 1255179848.5084 - val_acc: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7031674185.5474 - acc: 0.0000e+00 - val_loss: 1255030669.2768 - val_acc: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7031306046.3281 - acc: 0.0000e+00 - val_loss: 1254881589.2944 - val_acc: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7030937913.1578 - acc: 0.0000e+00 - val_loss: 1254732413.0548 - val_acc: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7030569804.3785 - acc: 0.0000e+00 - val_loss: 1254583287.6786 - val_acc: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7030201708.4778 - acc: 0.0000e+00 - val_loss: 1254434154.4953 - val_acc: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7029833703.7992 - acc: 0.0000e+00 - val_loss: 1254285111.5851 - val_acc: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7029465673.2665 - acc: 0.0000e+00 - val_loss: 1254135981.3937 - val_acc: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7029097633.6588 - acc: 0.0000e+00 - val_loss: 1253986862.0015 - val_acc: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7028729693.7628 - acc: 0.0000e+00 - val_loss: 1253837771.2199 - val_acc: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7028361811.2336 - acc: 0.0000e+00 - val_loss: 1253688749.9080 - val_acc: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7027993713.0872 - acc: 0.0000e+00 - val_loss: 1253539687.8773 - val_acc: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7027625802.7527 - acc: 0.0000e+00 - val_loss: 1253390648.6136 - val_acc: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7027257854.2700 - acc: 0.0000e+00 - val_loss: 1253241566.2936 - val_acc: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7026889949.9834 - acc: 0.0000e+00 - val_loss: 1253092638.2469 - val_acc: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7026522129.1149 - acc: 0.0000e+00 - val_loss: 1252943582.2001 - val_acc: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7026154277.3168 - acc: 0.0000e+00 - val_loss: 1252794545.9284 - val_acc: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7025786390.8850 - acc: 0.0000e+00 - val_loss: 1252645532.9379 - val_acc: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7025418689.2389 - acc: 0.0000e+00 - val_loss: 1252496655.7546 - val_acc: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7025050751.5842 - acc: 0.0000e+00 - val_loss: 1252347655.3864 - val_acc: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7024682920.9581 - acc: 0.0000e+00 - val_loss: 1252198692.0906 - val_acc: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7024315194.8218 - acc: 0.0000e+00 - val_loss: 1252049817.2447 - val_acc: 0.0000e+00\n",
      "Epoch 281/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7023947408.7820 - acc: 0.0000e+00 - val_loss: 1251900859.9328 - val_acc: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7023579730.6495 - acc: 0.0000e+00 - val_loss: 1251751902.6209 - val_acc: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7023212080.0279 - acc: 0.0000e+00 - val_loss: 1251603078.6384 - val_acc: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7022844332.0372 - acc: 0.0000e+00 - val_loss: 1251454144.0935 - val_acc: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7022476650.2920 - acc: 0.0000e+00 - val_loss: 1251305265.0869 - val_acc: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7022108910.3015 - acc: 0.0000e+00 - val_loss: 1251156441.1045 - val_acc: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7021741331.6824 - acc: 0.0000e+00 - val_loss: 1251007554.9452 - val_acc: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7021373745.1601 - acc: 0.0000e+00 - val_loss: 1250858659.8101 - val_acc: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7021006128.2946 - acc: 0.0000e+00 - val_loss: 1250709880.7071 - val_acc: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7020638387.5236 - acc: 0.0000e+00 - val_loss: 1250561032.2747 - val_acc: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7020270851.1482 - acc: 0.0000e+00 - val_loss: 1250412228.0672 - val_acc: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7019903337.0186 - acc: 0.0000e+00 - val_loss: 1250263474.0687 - val_acc: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7019535772.3506 - acc: 0.0000e+00 - val_loss: 1250114641.7648 - val_acc: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7019168325.1479 - acc: 0.0000e+00 - val_loss: 1249965833.3966 - val_acc: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7018800773.3574 - acc: 0.0000e+00 - val_loss: 1249817123.1088 - val_acc: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7018433302.7392 - acc: 0.0000e+00 - val_loss: 1249668349.9898 - val_acc: 0.0000e+00\n",
      "Epoch 297/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 7018065817.0973 - acc: 0.0000e+00 - val_loss: 1249519676.7743 - val_acc: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7017698454.2882 - acc: 0.0000e+00 - val_loss: 1249370898.3258 - val_acc: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7017330976.6455 - acc: 0.0000e+00 - val_loss: 1249222129.3674 - val_acc: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7016963574.2253 - acc: 0.0000e+00 - val_loss: 1249073490.2323 - val_acc: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7016596139.5097 - acc: 0.0000e+00 - val_loss: 1248924781.7677 - val_acc: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7016228802.4566 - acc: 0.0000e+00 - val_loss: 1248776140.8093 - val_acc: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7015861507.3572 - acc: 0.0000e+00 - val_loss: 1248627427.5296 - val_acc: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7015494153.7170 - acc: 0.0000e+00 - val_loss: 1248478829.1132 - val_acc: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7015126890.8128 - acc: 0.0000e+00 - val_loss: 1248330175.0183 - val_acc: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7014759523.7087 - acc: 0.0000e+00 - val_loss: 1248181470.7144 - val_acc: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7014392212.0223 - acc: 0.0000e+00 - val_loss: 1248032886.6034 - val_acc: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7014025014.8742 - acc: 0.0000e+00 - val_loss: 1247884233.1629 - val_acc: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7013657766.9937 - acc: 0.0000e+00 - val_loss: 1247735708.8912 - val_acc: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7013290485.6493 - acc: 0.0000e+00 - val_loss: 1247587054.7962 - val_acc: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7012923296.8917 - acc: 0.0000e+00 - val_loss: 1247438538.8459 - val_acc: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7012556118.2819 - acc: 0.0000e+00 - val_loss: 1247289930.2849 - val_acc: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7012188978.4035 - acc: 0.0000e+00 - val_loss: 1247141424.4792 - val_acc: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7011821809.4521 - acc: 0.0000e+00 - val_loss: 1246992821.9021 - val_acc: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7011454637.3774 - acc: 0.0000e+00 - val_loss: 1246844369.4375 - val_acc: 0.0000e+00\n",
      "Epoch 316/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7011087603.6501 - acc: 0.0000e+00 - val_loss: 1246695807.4390 - val_acc: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7010720509.8210 - acc: 0.0000e+00 - val_loss: 1246547336.3682 - val_acc: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7010353416.5783 - acc: 0.0000e+00 - val_loss: 1246398813.9196 - val_acc: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7009986320.9941 - acc: 0.0000e+00 - val_loss: 1246250390.7202 - val_acc: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7009619332.3393 - acc: 0.0000e+00 - val_loss: 1246101860.4646 - val_acc: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7009252353.3439 - acc: 0.0000e+00 - val_loss: 1245953431.2812 - val_acc: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7008885309.2726 - acc: 0.0000e+00 - val_loss: 1245804944.7363 - val_acc: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7008518262.3714 - acc: 0.0000e+00 - val_loss: 1245656553.7940 - val_acc: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7008151343.2794 - acc: 0.0000e+00 - val_loss: 1245508068.4178 - val_acc: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7007784437.7494 - acc: 0.0000e+00 - val_loss: 1245359709.2184 - val_acc: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7007417487.6319 - acc: 0.0000e+00 - val_loss: 1245211388.9145 - val_acc: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7007050623.7602 - acc: 0.0000e+00 - val_loss: 1245062937.6187 - val_acc: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7006683782.3294 - acc: 0.0000e+00 - val_loss: 1244914597.5398 - val_acc: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7006316810.7468 - acc: 0.0000e+00 - val_loss: 1244766183.9708 - val_acc: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7005949972.0478 - acc: 0.0000e+00 - val_loss: 1244617866.0044 - val_acc: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7005583199.8865 - acc: 0.0000e+00 - val_loss: 1244469461.4112 - val_acc: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7005216303.5254 - acc: 0.0000e+00 - val_loss: 1244321157.2359 - val_acc: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7004849484.9225 - acc: 0.0000e+00 - val_loss: 1244172918.8839 - val_acc: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7004482795.5928 - acc: 0.0000e+00 - val_loss: 1244024520.7889 - val_acc: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7004115975.9172 - acc: 0.0000e+00 - val_loss: 1243876270.4690 - val_acc: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7003749238.2926 - acc: 0.0000e+00 - val_loss: 1243727959.7955 - val_acc: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7003382557.3535 - acc: 0.0000e+00 - val_loss: 1243579712.9817 - val_acc: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7003015837.9729 - acc: 0.0000e+00 - val_loss: 1243431475.7984 - val_acc: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7002649161.1304 - acc: 0.0000e+00 - val_loss: 1243283220.1490 - val_acc: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7002282383.9918 - acc: 0.0000e+00 - val_loss: 1243134988.9496 - val_acc: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7001915828.5192 - acc: 0.0000e+00 - val_loss: 1242986796.6457 - val_acc: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7001549187.1890 - acc: 0.0000e+00 - val_loss: 1242838543.3338 - val_acc: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7001182582.2509 - acc: 0.0000e+00 - val_loss: 1242690368.3272 - val_acc: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7000816018.7770 - acc: 0.0000e+00 - val_loss: 1242542199.9591 - val_acc: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7000449404.9599 - acc: 0.0000e+00 - val_loss: 1242393992.1812 - val_acc: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 7000082864.6082 - acc: 0.0000e+00 - val_loss: 1242245845.2710 - val_acc: 0.0000e+00\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 15ms/step - loss: 6999716341.6240 - acc: 0.0000e+00 - val_loss: 1242097718.7904 - val_acc: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6999349789.2714 - acc: 0.0000e+00 - val_loss: 1241949507.3660 - val_acc: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6998983268.9198 - acc: 0.0000e+00 - val_loss: 1241801425.1103 - val_acc: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6998616893.7440 - acc: 0.0000e+00 - val_loss: 1241653341.0314 - val_acc: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6998250319.2448 - acc: 0.0000e+00 - val_loss: 1241505172.6633 - val_acc: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6997883851.8701 - acc: 0.0000e+00 - val_loss: 1241357090.4076 - val_acc: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6997517502.4512 - acc: 0.0000e+00 - val_loss: 1241209033.9109 - val_acc: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6997151035.6619 - acc: 0.0000e+00 - val_loss: 1241060910.4222 - val_acc: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6996784703.5114 - acc: 0.0000e+00 - val_loss: 1240912868.2308 - val_acc: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6996418258.4773 - acc: 0.0000e+00 - val_loss: 1240764849.9752 - val_acc: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6996051960.7664 - acc: 0.0000e+00 - val_loss: 1240616846.6793 - val_acc: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6995685634.8589 - acc: 0.0000e+00 - val_loss: 1240468754.2790 - val_acc: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6995319326.2208 - acc: 0.0000e+00 - val_loss: 1240320788.0555 - val_acc: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6994953059.7294 - acc: 0.0000e+00 - val_loss: 1240172799.7195 - val_acc: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6994586777.6278 - acc: 0.0000e+00 - val_loss: 1240024828.6808 - val_acc: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6994220444.1102 - acc: 0.0000e+00 - val_loss: 1239876775.1760 - val_acc: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6993854253.9141 - acc: 0.0000e+00 - val_loss: 1239728838.8722 - val_acc: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6993487968.0069 - acc: 0.0000e+00 - val_loss: 1239580899.5763 - val_acc: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6993121752.4430 - acc: 0.0000e+00 - val_loss: 1239433015.9591 - val_acc: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6992755667.5182 - acc: 0.0000e+00 - val_loss: 1239284993.5427 - val_acc: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6992389411.1724 - acc: 0.0000e+00 - val_loss: 1239137113.4317 - val_acc: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6992023285.0744 - acc: 0.0000e+00 - val_loss: 1238989233.9752 - val_acc: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6991657204.5396 - acc: 0.0000e+00 - val_loss: 1238841327.5909 - val_acc: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6991291049.7577 - acc: 0.0000e+00 - val_loss: 1238693467.2549 - val_acc: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6990924912.9276 - acc: 0.0000e+00 - val_loss: 1238545539.9270 - val_acc: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6990558922.0538 - acc: 0.0000e+00 - val_loss: 1238397699.3660 - val_acc: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "164/164 [==============================] - 2s 14ms/step - loss: 6990192865.4205 - acc: 0.0000e+00 - val_loss: 1238249870.1183 - val_acc: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6989826853.4727 - acc: 0.0000e+00 - val_loss: 1238102034.8868 - val_acc: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6989460755.4711 - acc: 0.0000e+00 - val_loss: 1237954275.6231 - val_acc: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6989094800.8901 - acc: 0.0000e+00 - val_loss: 1237806448.1987 - val_acc: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6988728727.1836 - acc: 0.0000e+00 - val_loss: 1237658597.4931 - val_acc: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6988362829.7742 - acc: 0.0000e+00 - val_loss: 1237510809.4785 - val_acc: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6987996933.2434 - acc: 0.0000e+00 - val_loss: 1237363068.1665 - val_acc: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6987631005.9793 - acc: 0.0000e+00 - val_loss: 1237215289.7823 - val_acc: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6987265046.4220 - acc: 0.0000e+00 - val_loss: 1237067533.5106 - val_acc: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6986899132.1334 - acc: 0.0000e+00 - val_loss: 1236919858.0219 - val_acc: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6986533287.1154 - acc: 0.0000e+00 - val_loss: 1236772106.5654 - val_acc: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6986167413.6098 - acc: 0.0000e+00 - val_loss: 1236624419.1088 - val_acc: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6985801652.4972 - acc: 0.0000e+00 - val_loss: 1236476633.5720 - val_acc: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6985435789.1361 - acc: 0.0000e+00 - val_loss: 1236328974.2118 - val_acc: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6985070087.5366 - acc: 0.0000e+00 - val_loss: 1236181275.9562 - val_acc: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6984704261.5432 - acc: 0.0000e+00 - val_loss: 1236033655.4916 - val_acc: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6984338534.1868 - acc: 0.0000e+00 - val_loss: 1235885951.2520 - val_acc: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6983972774.7322 - acc: 0.0000e+00 - val_loss: 1235738366.6910 - val_acc: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6983607076.0596 - acc: 0.0000e+00 - val_loss: 1235590730.6121 - val_acc: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6983241415.3391 - acc: 0.0000e+00 - val_loss: 1235443119.1234 - val_acc: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6982875779.3987 - acc: 0.0000e+00 - val_loss: 1235295519.6026 - val_acc: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6982510056.3351 - acc: 0.0000e+00 - val_loss: 1235147908.1140 - val_acc: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "164/164 [==============================] - 2s 15ms/step - loss: 6982144406.6377 - acc: 0.0000e+00 - val_loss: 1235000350.9949 - val_acc: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "157/164 [===========================>..] - ETA: 0s - loss: 6423248914.5478 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "sp_activator = StockPredictionActivator(get_simple_lstm(pre_data_length=pre_data_length, num_features=6, output_dim=1))\n",
    "sp_activator.train(train_data=train_stock_data, validation_data=test_stock_data, \n",
    "                   epochs=1000, batch_size=64, verbose=1, validation_split=0.1,\n",
    "                  pre_data_length=pre_data_length, pro_data_length=pro_data_length, process_data=process_data,\n",
    "                  step_estimator=get_estimated_steps, generator=data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ahahaha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, tester, batch_size=64, **kwargs):\n",
    "    pre_data_length = kwargs['pre_data_length']\n",
    "    pro_data_length = kwargs['pro_data_length']\n",
    "    process_data = kwargs['process_data']\n",
    "    step_estimator = kwargs['step_estimator']\n",
    "    generator = kwargs['generator']\n",
    "    test_steps, test_steps_per_epoch = step_estimator(test_data, pre_data_length, pro_data_length, batch_size)\n",
    "    print('Total Samples:', test_steps)\n",
    "\n",
    "    \"\"\"\n",
    "    모든 회사를 한꺼번에 넣으면 각 미니배치마다 랜덤 회사를 가지고 오기 때문에, predict 된 내역과 실제 y를 비교 불가능하게 된다.\n",
    "    따라서 한 회사씩 predict하여 그 순서가 회사 -> 시간순으로 예측되도록 한다.\n",
    "    \"\"\"\n",
    "    all_predicted = list()\n",
    "    for td in test_data:\n",
    "        test_steps, test_steps_per_epoch = step_estimator([td], pre_data_length, pro_data_length, batch_size)\n",
    "        predicted = model.predict_generator(generator=generator(all_stock_data=[td], \n",
    "                                                                batch_size=batch_size, \n",
    "                                                                pre_data_length=pre_data_length, \n",
    "                                                                pro_data_length=pro_data_length,\n",
    "                                                                process_data=process_data),\n",
    "                                            steps=test_steps_per_epoch)\n",
    "        all_predicted.extend(predicted)\n",
    "    \n",
    "    tester(test_data, predicted, pre_data_length=pre_data_length, pro_data_length=pro_data_length)\n",
    "    return all_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regression_tester(test_data, predicted, **kwargs):\n",
    "    pre_data_length = kwargs['pre_data_length']\n",
    "    pro_data_length = kwargs['pro_data_length']\n",
    "    \n",
    "    y_data = list()\n",
    "    for td in test_data:\n",
    "        y_data.extend(td[pre_data_length:, 0])\n",
    "    \n",
    "    for y, y_hat in zip(y_data, predicted):\n",
    "        print(y, y_hat)\n",
    "    \n",
    "all_predicted = test(model=sp_activator.model, test_data=test_stock_data, tester=regression_tester, batch_size=64,\n",
    "                     pre_data_length=pre_data_length, pro_data_length=pro_data_length, process_data=process_data,\n",
    "                     step_estimator=get_estimated_steps, generator=data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50.0]\n",
      " [350.0]\n",
      " [600.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [400.0]\n",
      " [800.0]\n",
      " [1500.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [500.0]\n",
      " [450.0]\n",
      " [0.0]\n",
      " [800.0]\n",
      " [500.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [900.0]\n",
      " [350.0]\n",
      " [0.0]\n",
      " [900.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [900.0]\n",
      " [700.0]\n",
      " [150.0]\n",
      " [0.0]\n",
      " [1400.0]\n",
      " [500.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [1500.0]\n",
      " [800.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [1000.0]\n",
      " [50.0]\n",
      " [600.0]\n",
      " [300.0]\n",
      " [500.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [600.0]\n",
      " [50.0]\n",
      " [150.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [400.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [1100.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [700.0]\n",
      " [100.0]]\n",
      "[[0.0]\n",
      " [150.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [250.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [250.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [400.0]\n",
      " [0.0]\n",
      " [800.0]\n",
      " [50.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [900.0]\n",
      " [150.0]\n",
      " [150.0]\n",
      " [600.0]\n",
      " [800.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [1000.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [300.0]\n",
      " [300.0]\n",
      " [150.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [100.0]]\n",
      "[[200.0]\n",
      " [200.0]\n",
      " [400.0]\n",
      " [800.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [500.0]\n",
      " [600.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [1100.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [1300.0]\n",
      " [700.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [500.0]\n",
      " [450.0]\n",
      " [250.0]\n",
      " [200.0]\n",
      " [500.0]\n",
      " [500.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [1300.0]\n",
      " [450.0]\n",
      " [100.0]\n",
      " [2000.0]\n",
      " [2100.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [900.0]\n",
      " [0.0]\n",
      " [350.0]\n",
      " [100.0]\n",
      " [600.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [400.0]\n",
      " [500.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [1000.0]\n",
      " [0.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [250.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [800.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [450.0]\n",
      " [300.0]\n",
      " [0.0]]\n",
      "[[0.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [250.0]\n",
      " [400.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [1000.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [350.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [500.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [500.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [300.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [0.0]]\n",
      "[[200.0]\n",
      " [450.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [450.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [350.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [16100.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [3400.0]\n",
      " [250.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [1000.0]\n",
      " [0.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [1700.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [700.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [600.0]\n",
      " [900.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [1000.0]\n",
      " [1000.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [900.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [550.0]\n",
      " [600.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [700.0]\n",
      " [300.0]\n",
      " [400.0]\n",
      " [250.0]\n",
      " [500.0]\n",
      " [1400.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [900.0]\n",
      " [200.0]\n",
      " [450.0]\n",
      " [550.0]\n",
      " [100.0]\n",
      " [900.0]]\n",
      "[[100.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [400.0]\n",
      " [700.0]\n",
      " [300.0]\n",
      " [250.0]\n",
      " [50.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [400.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [250.0]\n",
      " [150.0]\n",
      " [250.0]\n",
      " [100.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [150.0]\n",
      " [200.0]\n",
      " [350.0]\n",
      " [1300.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [900.0]\n",
      " [150.0]\n",
      " [350.0]\n",
      " [350.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [250.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [300.0]\n",
      " [1200.0]\n",
      " [250.0]\n",
      " [300.0]\n",
      " [600.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [700.0]\n",
      " [400.0]\n",
      " [150.0]\n",
      " [150.0]\n",
      " [500.0]\n",
      " [500.0]]\n",
      "[[0.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [500.0]\n",
      " [200.0]\n",
      " [350.0]\n",
      " [650.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [550.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [150.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [200.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [150.0]\n",
      " [350.0]\n",
      " [400.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [550.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [1050.0]\n",
      " [200.0]\n",
      " [600.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [300.0]\n",
      " [250.0]\n",
      " [200.0]\n",
      " [650.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [850.0]\n",
      " [500.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [450.0]\n",
      " [300.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [350.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [200.0]]\n",
      "[[50.0]\n",
      " [250.0]\n",
      " [500.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [1250.0]\n",
      " [100.0]\n",
      " [250.0]\n",
      " [0.0]\n",
      " [1750.0]\n",
      " [400.0]\n",
      " [350.0]\n",
      " [0.0]\n",
      " [0.0]\n",
      " [400.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [950.0]\n",
      " [200.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [400.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [250.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [400.0]\n",
      " [100.0]\n",
      " [700.0]\n",
      " [500.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [500.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [500.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [200.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [350.0]\n",
      " [500.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [250.0]\n",
      " [800.0]]\n",
      "[[300.0]\n",
      " [0.0]\n",
      " [600.0]\n",
      " [700.0]\n",
      " [300.0]\n",
      " [200.0]\n",
      " [350.0]\n",
      " [800.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [300.0]\n",
      " [400.0]\n",
      " [450.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [550.0]\n",
      " [900.0]\n",
      " [50.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [600.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [550.0]\n",
      " [300.0]\n",
      " [250.0]\n",
      " [50.0]\n",
      " [300.0]\n",
      " [300.0]\n",
      " [350.0]\n",
      " [0.0]\n",
      " [50.0]\n",
      " [1600.0]\n",
      " [50.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [150.0]\n",
      " [300.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [50.0]\n",
      " [0.0]\n",
      " [400.0]\n",
      " [300.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [100.0]]\n",
      "[[250.0]\n",
      " [50.0]\n",
      " [200.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [600.0]\n",
      " [300.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [850.0]\n",
      " [400.0]\n",
      " [200.0]\n",
      " [150.0]\n",
      " [1750.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [2250.0]\n",
      " [100.0]\n",
      " [150.0]\n",
      " [200.0]\n",
      " [1300.0]\n",
      " [400.0]\n",
      " [0.0]\n",
      " [300.0]\n",
      " [1100.0]\n",
      " [300.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [250.0]\n",
      " [1200.0]\n",
      " [200.0]\n",
      " [50.0]\n",
      " [700.0]\n",
      " [100.0]\n",
      " [0.0]\n",
      " [100.0]\n",
      " [1200.0]\n",
      " [400.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [800.0]\n",
      " [300.0]\n",
      " [350.0]\n",
      " [150.0]\n",
      " [1450.0]\n",
      " [700.0]\n",
      " [400.0]\n",
      " [50.0]\n",
      " [1150.0]\n",
      " [200.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [100.0]\n",
      " [500.0]\n",
      " [150.0]\n",
      " [100.0]\n",
      " [100.0]\n",
      " [50.0]\n",
      " [150.0]\n",
      " [150.0]\n",
      " [1150.0]\n",
      " [950.0]]\n"
     ]
    }
   ],
   "source": [
    "loader = data_loader(all_stock_data=test_stock_data, batch_size=64, \n",
    "                     pre_data_length=20, pro_data_length=1, \n",
    "                     process_data=process_data)\n",
    "for idx, (x_batch, y_batch) in enumerate(loader):\n",
    "    if idx == 10:\n",
    "        break\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[14800.0, 500.0, 15250.0, 15400.0, 14650.0, 89378.0],\n",
       "        [14900.0, 100.0, 14900.0, 15050.0, 14750.0, 23480.0],\n",
       "        [14800.0, 100.0, 15100.0, 15100.0, 14700.0, 28288.0],\n",
       "        ...,\n",
       "        [12650.0, 400.0, 13050.0, 13100.0, 12550.0, 45778.0],\n",
       "        [12650.0, 0.0, 12550.0, 12800.0, 12400.0, 23579.0],\n",
       "        [12900.0, 250.0, 12650.0, 12900.0, 12550.0, 19363.0]], dtype=object),\n",
       " array([[13600.0, 350.0, 13800.0, 13950.0, 13550.0, 38914.0],\n",
       "        [13900.0, 300.0, 13550.0, 13950.0, 13450.0, 44309.0],\n",
       "        [14050.0, 150.0, 13900.0, 14250.0, 13900.0, 59080.0],\n",
       "        ...,\n",
       "        [11750.0, 200.0, 11900.0, 12050.0, 11650.0, 55984.0],\n",
       "        [12000.0, 250.0, 11900.0, 12150.0, 11750.0, 18309.0],\n",
       "        [12150.0, 150.0, 12000.0, 12250.0, 12000.0, 14377.0]], dtype=object),\n",
       " array([[43700.0, 8400.0, 52100.0, 67000.0, 41100.0, 506129.0],\n",
       "        [48000.0, 4300.0, 44300.0, 52700.0, 44150.0, 608556.0],\n",
       "        [54600.0, 6600.0, 49500.0, 56200.0, 47750.0, 258515.0],\n",
       "        ...,\n",
       "        [43650.0, 1250.0, 45000.0, 45000.0, 43300.0, 4663.0],\n",
       "        [44500.0, 850.0, 43650.0, 44700.0, 43650.0, 3427.0],\n",
       "        [44100.0, 400.0, 44550.0, 44600.0, 43150.0, 5300.0]], dtype=object),\n",
       " array([[76000.0, 700.0, 76100.0, 77700.0, 75400.0, 2730.0],\n",
       "        [76000.0, 0.0, 76700.0, 76700.0, 75400.0, 3093.0],\n",
       "        [73500.0, 2500.0, 76700.0, 77100.0, 73300.0, 2183.0],\n",
       "        ...,\n",
       "        [45400.0, 1000.0, 46950.0, 46950.0, 45300.0, 13499.0],\n",
       "        [48000.0, 2600.0, 45700.0, 48700.0, 45400.0, 12647.0],\n",
       "        [49250.0, 1250.0, 48150.0, 49900.0, 47000.0, 7213.0]], dtype=object)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainwave_eeg",
   "language": "python",
   "name": "brainwave_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

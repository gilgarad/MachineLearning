{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence2Sequence 논문 구현\n",
    "# 0.baseline: simple sequence2sequence (encoder hidden state or output -> decoder initial state)\n",
    "# 1.논문: Sequence to Sequence Learning with Neural Networks \n",
    "## https://arxiv.org/abs/1409.3215\n",
    "\n",
    "# 2. 논문: Luong Attention \n",
    "## https://arxiv.org/abs/1508.04025\n",
    "\n",
    "# 3. 논문: Bahdanau Attention\n",
    "## https://arxiv.org/abs/1409.0473\n",
    "\n",
    "# 4. 논문: Transformer\n",
    "## https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.layers import Input, GRU, Dense, Activation, Lambda, LSTM, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loader\n",
    "## 미리 구현된 것 복사&붙여넣기\n",
    "## http://121.140.2.142:8888/notebooks/test_notes/preprocess/nlp_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 데이터도 가지고 올 수 있도록 로직을 약간 수정했습니다.\n",
    "## Decoder는 \\<start>와 \\<end> 태그를 문장의 시작과 끝에 자동으로 넣을 수 있도록 변경하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "    \n",
    "    \"\"\"\n",
    "    1. 다음의 두 함수는 소단위로 구현된 아래 함수들[2]를 응용하는 함수입니다. 필요에 따라 추후에 merge하면 좋겠지요 ^^\n",
    "    \"\"\"\n",
    "    def get_classification_data(self, data_root_path, data_type, max_seq_length):\n",
    "        # 1. Read Data\n",
    "        data_path_list = listdir(data_root_path)\n",
    "        data_path_list = [join(data_root_path, d) for d in data_path_list]\n",
    "        all_documents, all_documents_names = self.read_data(data_path_list=data_path_list, data_type=data_type)\n",
    "#         print(all_documents_names)\n",
    "#         print(all_documents)\n",
    "\n",
    "        # 2. Tokenize data\n",
    "        all_tokenized_documents = list()\n",
    "        for documents in all_documents:\n",
    "            tokenized_documents = self.tokenize(documents=documents, token_type='word')\n",
    "            all_tokenized_documents.append(tokenized_documents)\n",
    "        print(all_tokenized_documents)\n",
    "        \n",
    "        # 3. token to index\n",
    "        token2idx_dict = dict()\n",
    "        token_counter_dict = dict()\n",
    "        all_indiced_documents = list()\n",
    "\n",
    "        for tokenized_documents in all_tokenized_documents:\n",
    "            indiced_documents, token2idx_dict, token_counter_dict = self.token2idx(tokenized_documents=tokenized_documents, \n",
    "                                                                              token2idx_dict=token2idx_dict, \n",
    "                                                                              token_counter_dict=token_counter_dict)\n",
    "            all_indiced_documents.append(indiced_documents)\n",
    "        \n",
    "        # 4. padd & formatting\n",
    "        all_padded_documents = list()\n",
    "        for indiced_documnets in all_indiced_documents:\n",
    "            padded_documents = self.pad_format(indiced_documnets, max_seq_length=max_seq_length)\n",
    "            all_padded_documents.append(padded_documents)\n",
    "        \n",
    "        # 5. Make classification dataset\n",
    "        label2idx = dict()\n",
    "        x_data = list()\n",
    "        y_data = list()\n",
    "\n",
    "        for padded_documents, label in zip(all_padded_documents, all_documents_names):\n",
    "            _x_data, _y_data, label2idx =self. map_document_label(documents=padded_documents, label=label, label2idx=label2idx)\n",
    "            x_data.append(_x_data)\n",
    "            y_data.append(_y_data)\n",
    "\n",
    "        x_data = np.concatenate(x_data, axis=0)\n",
    "        y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "        for idx, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "            if idx == 10:\n",
    "                break\n",
    "#             print('label:', y)\n",
    "#             print('data:', x)\n",
    "        \n",
    "        return x_data, y_data\n",
    "        \n",
    "        \n",
    "    def get_encoder_decoder_data(self, encoder_root_path, decoder_root_path, data_type, encoder_language, decoder_langeuage,\n",
    "                                 max_enc_seq_length, max_dec_seq_length, \n",
    "                                 encoder_token_counter_dict=dict(), \n",
    "                                 decoder_token_counter_dict=dict()): # token 추가하는 token_counter_dict는 개발하지 않음\n",
    "        # 1. Read Data\n",
    "        encoder_path_list = listdir(encoder_root_path)\n",
    "        encoder_path_list = [join(encoder_root_path, d) for d in encoder_path_list]\n",
    "        \n",
    "        decoder_path_list = listdir(decoder_root_path)\n",
    "        decoder_path_list = [join(decoder_root_path, d) for d in decoder_path_list]\n",
    "        \n",
    "        data_path_list = np.concatenate([encoder_path_list, decoder_path_list], axis=0)\n",
    "        ########## same logic below\n",
    "        all_documents, all_documents_names = self.read_data(data_path_list=data_path_list, data_type=data_type)\n",
    "#         print(all_documents_names)\n",
    "#         print(all_documents)\n",
    "\n",
    "        # 2. Tokenize data\n",
    "        all_tokenized_documents = list()\n",
    "        for documents in all_documents:\n",
    "            tokenized_documents = self.tokenize(documents=documents, token_type='word')\n",
    "            all_tokenized_documents.append(tokenized_documents)\n",
    "        print(all_tokenized_documents)\n",
    "        \n",
    "        ########## same logic above\n",
    "        \n",
    "        # 3. token to index\n",
    "        encoder_token2idx_dict = dict()\n",
    "        encoder_token_counter_dict = dict()\n",
    "        encoder_all_indiced_documents = list()\n",
    "        encoder_documents = list()\n",
    "        \n",
    "        for documents, tokenized_documents in zip(all_documents[:len(encoder_path_list)], all_tokenized_documents[:len(encoder_path_list)]):\n",
    "            indiced_documents, encoder_token2idx_dict, encoder_token_counter_dict = \\\n",
    "                    self.token2idx(tokenized_documents=tokenized_documents,\n",
    "                                   add_tag=False,\n",
    "                                   token2idx_dict=encoder_token2idx_dict, \n",
    "                                   token_counter_dict=encoder_token_counter_dict)\n",
    "            encoder_all_indiced_documents.append(indiced_documents)\n",
    "            encoder_documents.append(documents)\n",
    "        \n",
    "        \n",
    "        decoder_token2idx_dict = dict()\n",
    "        decoder_token_counter_dict = dict()\n",
    "        decoder_all_indiced_documents = list()\n",
    "        decoder_documents = list()\n",
    "\n",
    "        for documents, tokenized_documents in zip(all_documents[len(encoder_path_list):], all_tokenized_documents[len(encoder_path_list):]):\n",
    "            indiced_documents, decoder_token2idx_dict, decoder_token_counter_dict = \\\n",
    "                    self.token2idx(tokenized_documents=tokenized_documents,\n",
    "                                   add_tag=True,\n",
    "                                   token2idx_dict=decoder_token2idx_dict, \n",
    "                                   token_counter_dict=decoder_token_counter_dict)\n",
    "            decoder_all_indiced_documents.append(indiced_documents)\n",
    "            decoder_documents.append(documents)\n",
    "        \n",
    "        # 4. padd & formatting\n",
    "        encoder_all_padded_documents = list()\n",
    "        for indiced_documnets in encoder_all_indiced_documents:\n",
    "            padded_documents = self.pad_format(indiced_documnets, max_seq_length=max_enc_seq_length)\n",
    "            encoder_all_padded_documents.append(padded_documents)\n",
    "            \n",
    "        decoder_all_padded_documents = list()\n",
    "        for indiced_documnets in decoder_all_indiced_documents:\n",
    "            padded_documents = self.pad_format(indiced_documnets, max_seq_length=max_enc_seq_length)\n",
    "            decoder_all_padded_documents.append(padded_documents)\n",
    "            \n",
    "        # 5. Make encoder decoder dataset\n",
    "        encoder_inputs = np.concatenate(encoder_all_padded_documents, axis=0)\n",
    "        decoder_inputs = list()\n",
    "        decoder_outputs = list()\n",
    "\n",
    "        for padded_documents in decoder_all_padded_documents:\n",
    "            _x_data, _y_data = self.map_document_ae(documents=padded_documents)\n",
    "            decoder_inputs.append(_x_data)\n",
    "            decoder_outputs.append(_y_data)\n",
    "\n",
    "        decoder_inputs = np.concatenate(decoder_inputs, axis=0)\n",
    "        decoder_outputs = np.concatenate(decoder_outputs, axis=0)\n",
    "        \n",
    "        encoder_documents = np.concatenate(encoder_documents, axis=0)\n",
    "        decoder_documents = np.concatenate(decoder_documents, axis=0)\n",
    "        \n",
    "        return [encoder_inputs, decoder_inputs, decoder_outputs], [encoder_documents, encoder_token2idx_dict, encoder_token_counter_dict,\n",
    "                                                                  decoder_documents, decoder_token2idx_dict, decoder_token_counter_dict]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    2. 아래의 함수들은 위에 구현된 함수들 복사 붙여넣기 후 파라미터에 가장 앞에 self를 추가해줍니다 ^^\n",
    "    \"\"\"\n",
    "    def read_data(self, data_path_list, data_type):\n",
    "        all_documents = list()\n",
    "        all_documents_names = list()\n",
    "        for data_path in data_path_list:\n",
    "            if 'numpy' == data_type: # Path(data_path).suffix\n",
    "                documents = np.load(data_path)\n",
    "            elif 'text' == data_type:\n",
    "                documents = list()\n",
    "                with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                    new_data_lines = f.read().split('\\n')\n",
    "                    documents.extend(new_data_lines)\n",
    "            else:\n",
    "                print('Proper data_type is not presented.')\n",
    "            all_documents.append(documents)\n",
    "            all_documents_names.append(Path(data_path).stem)\n",
    "        return all_documents, all_documents_names\n",
    "    \n",
    "    def tokenize(self, documents, token_type):\n",
    "        if token_type == 'char':\n",
    "            return [char for document in documents for char in document]\n",
    "        elif token_type == 'word': # token_type == word\n",
    "            return [self.okt.morphs(document, norm=False, stem=False) for document in documents]\n",
    "        elif token_type == 'ngram': # 'ngram'\n",
    "            tokenized_sentence = sentence.split(' ')\n",
    "            if kor_tokenizer_max_word_char == -1:\n",
    "                return [word for word in tokenized_sentence]\n",
    "            else:\n",
    "                return [word[:kor_tokenizer_max_word_char] for word in tokenized_sentence]\n",
    "        else:\n",
    "            print('Not implemented token type:', token_type)\n",
    "    \n",
    "    def token2idx(self, tokenized_documents, add_tag=False, token2idx_dict=dict(), token_counter_dict=dict()):\n",
    "        \"\"\"\n",
    "        1. will change token to idx and 2. token2idx dictionary and 3. counted token dictionary\n",
    "        \"\"\"\n",
    "        indiced_documents = list()\n",
    "\n",
    "        if len(token2idx_dict) == 0:\n",
    "            token2idx_dict['<pad>'] = 0\n",
    "            token2idx_dict['<start>'] = 1\n",
    "            token2idx_dict['<end>'] = 2\n",
    "            token2idx_dict['<unk>'] = 3\n",
    "\n",
    "        for tokenized_document in tokenized_documents:\n",
    "            indiced_document = list()\n",
    "            for token in tokenized_document:\n",
    "                if token not in token2idx_dict:\n",
    "                    token2idx_dict[token] = len(list(token2idx_dict.keys()))\n",
    "                if token not in token_counter_dict:\n",
    "                    token_counter_dict[token] = 0\n",
    "                token_counter_dict[token] += 1\n",
    "\n",
    "                indiced_document.append(token2idx_dict[token])\n",
    "            if add_tag:\n",
    "                indiced_document.insert(0, token2idx_dict['<start>'])\n",
    "                indiced_document.append(token2idx_dict['<end>'])\n",
    "            indiced_documents.append(indiced_document)\n",
    "        return indiced_documents, token2idx_dict, token_counter_dict\n",
    "    \n",
    "    def pad_format(self, indiced_documents, max_seq_length):\n",
    "        padded_data = np.empty(shape=(0, max_seq_length))\n",
    "\n",
    "        for indiced_document in indiced_documents:\n",
    "            np_transformed = np.zeros(shape=(max_seq_length, ))\n",
    "            for idx, index in enumerate(indiced_document):\n",
    "                if idx == max_seq_length:\n",
    "                    break\n",
    "                np_transformed[idx] = index\n",
    "            padded_data = np.insert(padded_data, padded_data.shape[0], np_transformed, axis=0)\n",
    "\n",
    "        return padded_data\n",
    "    \n",
    "    def map_document_label(self, documents, label, label2idx=dict()):\n",
    "        x_data = documents\n",
    "        y_data = list()\n",
    "        idx = len(label2idx)\n",
    "\n",
    "        if label not in label2idx:\n",
    "            label2idx[label] = idx\n",
    "        labels = [label2idx[label]] * documents.shape[0]\n",
    "        y_data.extend(labels)\n",
    "\n",
    "        return x_data, y_data, label2idx\n",
    "    \n",
    "    def map_document_ae(self, documents):\n",
    "        x_data = documents\n",
    "        y_data = documents[:, 1:]\n",
    "        y_data = np.insert(y_data, y_data.shape[1], 0, axis=1)\n",
    "        return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_enc_seq_length = 50\n",
    "max_dec_seq_length = 50\n",
    "\n",
    "train_data_path = '/data1/translation_data/train'\n",
    "test_data_path = '/data1/translation_data/test'\n",
    "val_data_path = '/data1/translation_data/dev_validation'\n",
    "\n",
    "encoder_root_path = join(train_data_path, 'english')\n",
    "decoder_root_path = join(train_data_path, 'korean')\n",
    "\n",
    "seq2seq_dataset = Dataset()\n",
    "[encoder_inputs, decoder_inputs, decoder_outputs], [encoder_documents, encoder_token2idx_dict, encoder_token_counter_dict,\n",
    "                                                                  decoder_documents, decoder_token2idx_dict, decoder_token_counter_dict] = seq2seq_dataset.get_encoder_decoder_data(\n",
    "    encoder_root_path=encoder_root_path, decoder_root_path=decoder_root_path, \n",
    "    data_type='text', encoder_language='english', decoder_langeuage='korean',\n",
    "    max_enc_seq_length=max_enc_seq_length, max_dec_seq_length=max_dec_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_inputs, test_encoder_inputs, \\\n",
    "train_decoder_inputs, test_decoder_inputs, \\\n",
    "train_decoder_outputs, test_decoder_outputs = train_test_split(encoder_inputs, decoder_inputs, decoder_outputs, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(encoder_token_counter_dict)\n",
    "num_decoder_tokens = len(decoder_token_counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequence 2 Sequence model\n",
    "## Embedding 참조: http://121.140.2.142:8888/notebooks/test_notes/keras/Embedding%20Layer%20%26%20Padding.ipynb\n",
    "## RNN 기본 참조: http://121.140.2.142:8888/notebooks/test_notes/keras/RNN%20-%20LSTM%2C%20GRU.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-0. Fit generator for each batch to transform final output as one hot\n",
    "# 모든 데이터셋을 전부 one hot으로 미리 만들어 놓으면 메모리 에러 발생 가능성이 높기 때문에... 매 배치마다 output 데이터를 one hot으로 실시간으로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_generator(x_data, y_data, num_decoder_tokens, batch_size=16):\n",
    "    '''\n",
    "    Return a random from x_data, y_data\n",
    "    '''\n",
    "    while True:\n",
    "        samples_per_mini_epoch = x_data[0].shape[0]\n",
    "        number_of_steps = np.ceil(samples_per_mini_epoch / batch_size).astype(int)\n",
    "        data_idx = list(range(number_of_steps))\n",
    "\n",
    "        while len(data_idx) > 0:\n",
    "            # choose batch_size random images / labels from the data\n",
    "\n",
    "            idx = random.choice(data_idx)\n",
    "            next_idx = min(samples_per_mini_epoch, idx + batch_size)\n",
    "            encoder_input = x_data[0][idx: next_idx]\n",
    "            decoder_input = x_data[1][idx: next_idx]\n",
    "            decoder_output = y_data[idx: next_idx]\n",
    "\n",
    "            converted_decoder_output = np.zeros(shape=(decoder_output.shape[0], decoder_output.shape[1], num_decoder_tokens))\n",
    "            for idx2, data in enumerate(decoder_output):\n",
    "                for idx3, d in enumerate(data):\n",
    "                    converted_decoder_output[idx2, idx3, int(d)] = 1\n",
    "\n",
    "            data_idx.remove(idx)\n",
    "            yield [encoder_input, decoder_input], converted_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, *args):\n",
    "#         print(len(args))\n",
    "        train_encoder_input, train_decoder_input, train_decoder_output = train_data\n",
    "        if validation_data is not None:\n",
    "            val_encoder_input, val_decoder_input, val_decoder_output = validation_data\n",
    "        \n",
    "#         mc = ModelCheckpoint('./save/s2s_{epoch:03d}.h5', save_weights_only=True, period=5)\n",
    "        \n",
    "        val_steps = 0\n",
    "        val_data = None\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            val_data = ([val_encoder_input, val_decoder_input], val_decoder_output)\n",
    "            val_steps = np.ceil(val_encoder_input.shape[0] / batch_size)\n",
    "            val_data = convert_generator(x_data=val_data[0], y_data=val_data[1], batch_size=batch_size,\n",
    "                                        num_decoder_tokens=num_decoder_tokens)\n",
    "\n",
    "        steps_per_epoch = np.ceil(train_encoder_input.shape[0] / batch_size)\n",
    "                \n",
    "        model.fit_generator(generator=convert_generator([train_encoder_input, train_decoder_input],\n",
    "                                                            train_decoder_output,\n",
    "                                                            num_decoder_tokens=num_decoder_tokens),\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 validation_data=val_data,\n",
    "                                 validation_steps=val_steps,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=verbose,\n",
    "#                                  callbacks=[mc]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "\n",
    "\n",
    "def get_seq2seq_base_model():\n",
    "    encoder_inputs = Input(shape=(max_enc_seq_length, ))\n",
    "    decoder_inputs = Input(shape=(max_dec_seq_length, ))\n",
    "    \n",
    "    encoder_embed_layer = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, input_length=max_enc_seq_length)\n",
    "    decoder_embed_layer = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, input_length=max_dec_seq_length)\n",
    "    \n",
    "    encoder_embeded_inputs = encoder_embed_layer(encoder_inputs)\n",
    "    decoder_embeded_inputs = encoder_embed_layer(decoder_inputs)\n",
    "    \n",
    "    encoder_layer = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, last_encoder_output, last_encoder_cell_state  = encoder_layer(encoder_embeded_inputs)\n",
    "    encoder_states = [last_encoder_output, last_encoder_cell_state]\n",
    "    \n",
    "    decoder_layer = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, last_decoder_output, last_decoder_cell_state  = decoder_layer(decoder_embeded_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    outputs = Dense(num_decoder_tokens, activation='softmax')(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 50, 128)      7117312     input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, 50, 64), (No 49408       embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, 50, 64), (No 49408       embedding_17[1][0]               \n",
      "                                                                 lstm_19[0][1]                    \n",
      "                                                                 lstm_19[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 50, 70626)    4590690     lstm_20[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,806,818\n",
      "Trainable params: 11,806,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "2648/2648 [==============================] - 1323s 500ms/step - loss: 0.0126 - acc: 0.4863 - val_loss: 1.1921e-07 - val_acc: 0.4932\n",
      "Epoch 2/5\n",
      "2648/2648 [==============================] - 1323s 500ms/step - loss: 1.1921e-07 - acc: 0.4856 - val_loss: 1.1921e-07 - val_acc: 0.4932\n",
      "Epoch 3/5\n",
      "2648/2648 [==============================] - 1327s 501ms/step - loss: 1.1921e-07 - acc: 0.4862 - val_loss: 1.1921e-07 - val_acc: 0.4932\n",
      "Epoch 4/5\n",
      "2648/2648 [==============================] - 1332s 503ms/step - loss: 1.1921e-07 - acc: 0.4860 - val_loss: 1.1921e-07 - val_acc: 0.4932\n",
      "Epoch 5/5\n",
      "2648/2648 [==============================] - 1333s 503ms/step - loss: 1.1921e-07 - acc: 0.4866 - val_loss: 1.1921e-07 - val_acc: 0.4932\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model = get_seq2seq_base_model()\n",
    "train(model=seq2seq_model, train_data=(train_encoder_inputs, train_decoder_inputs, train_decoder_outputs), \n",
    "      validation_data=(test_encoder_inputs, test_decoder_inputs, test_decoder_outputs), epochs=5, batch_size=32, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2. Sequence 2 Sequence (encoder last output as decoder first input) 하다 말아씀..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq2seq_general():\n",
    "    encoder_inputs = Input(shape=(max_enc_seq_length, ))\n",
    "    decoder_inputs = Input(shape=(max_dec_seq_length, ))\n",
    "    \n",
    "    encoder_embed_layer = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, input_length=max_enc_seq_length)\n",
    "    decoder_embed_layer = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, input_length=max_dec_seq_length)\n",
    "    \n",
    "    encoder_embeded_inputs = encoder_embed_layer(encoder_inputs)\n",
    "    decoder_embeded_inputs = encoder_embed_layer(decoder_inputs)\n",
    "    \n",
    "    encoder_layer1 = Bidirectional(LSTM(hidden_dim, return_sequences=True, return_state=True))\n",
    "    encoder_layer2 = Bidirectional(LSTM(hidden_dim, return_sequences=True, return_state=True))\n",
    "    encoder_layer3 = Bidirectional(LSTM(hidden_dim, return_sequences=True, return_state=True))\n",
    "    encoder_layer4 = Bidirectional(LSTM(hidden_dim, return_sequences=True, return_state=True))\n",
    "    encoder_outputs, last_encoder_output, last_encoder_cell_state  = encoder_layer1(encoder_embeded_inputs)\n",
    "    encoder_outputs, last_encoder_output, last_encoder_cell_state  = encoder_layer2(encoder_outputs)\n",
    "    encoder_outputs, last_encoder_output, last_encoder_cell_state  = encoder_layer3(encoder_outputs)\n",
    "    encoder_outputs, last_encoder_output, last_encoder_cell_state  = encoder_layer4(encoder_outputs)\n",
    "    \n",
    "    encoder_states = [last_encoder_output, last_encoder_cell_state]\n",
    "    \n",
    "    decoder_layer = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, last_decoder_output, last_decoder_cell_state  = decoder_layer(decoder_embeded_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    outputs = Dense(num_decoder_tokens, activation='softmax')(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b00d9b5823ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq2seq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_seq2seq_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m train(model=seq2seq_model, train_data=(train_encoder_inputs, train_decoder_inputs, train_decoder_outputs), \n\u001b[1;32m      3\u001b[0m       validation_data=(test_encoder_inputs, test_decoder_inputs, test_decoder_outputs), epochs=10, batch_size=64, verbose=1, validation_split=0.1)\n",
      "\u001b[0;32m<ipython-input-68-81d405a46c33>\u001b[0m in \u001b[0;36mget_seq2seq_general\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoder_layer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mencoder_layer4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_cell_state\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embeded_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_cell_state\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_encoder_cell_state\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "seq2seq_model = get_seq2seq_general()\n",
    "train(model=seq2seq_model, train_data=(train_encoder_inputs, train_decoder_inputs, train_decoder_outputs), \n",
    "      validation_data=(test_encoder_inputs, test_decoder_inputs, test_decoder_outputs), epochs=10, batch_size=64, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Attention Luong (Encoder에서 bidirectional 사용하는 것은 적용 안 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Concatenate, RepeatVector, Permute, Softmax, Multiply\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of last dimension is 0, then that means it is padded !\n",
    "def get_pad_index():\n",
    "    return Lambda(lambda x: K.cast(K.not_equal(K.sum(x, axis=-1, keepdims=True), 0), 'float32'))\n",
    "\n",
    "def get_last_outputs(inputs, outputs, dimension, seq_length):\n",
    "    if dimension == 2:\n",
    "        new_inputs = Reshape((seq_length, 1))(inputs)\n",
    "    else:\n",
    "        new_inputs = inputs\n",
    "    pad_index = get_pad_index()(new_inputs)\n",
    "    last_index = Lambda(lambda x: K.sum(x, axis=-2) - 1)(pad_index)\n",
    "\n",
    "    # LAST RELEVANT OUTPUT\n",
    "    # create the row index with tf.range\n",
    "    row_idx = Lambda(lambda x: tf.reshape(tf.range(tf.shape(x)[0]), (-1,1)))(last_index)\n",
    "\n",
    "    # stack with column index\n",
    "    idx = Lambda(lambda x: tf.stack([row_idx, K.cast(x, 'int32')], axis=-1))(last_index)\n",
    "    # extract the elements with gather_nd\n",
    "    last_outputs = Lambda(lambda x: tf.gather_nd(x, idx))(outputs)\n",
    "    \n",
    "    last_outputs = Reshape((hidden_dim, ))(last_outputs)\n",
    "    return pad_index, last_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "\n",
    "def get_seq2seq_luong_attention_model(gpu=0):\n",
    "    with K.tf.device('/gpu:' + str(gpu)):\n",
    "        encoder_inputs = Input(shape=(max_enc_seq_length, ))\n",
    "        decoder_inputs = Input(shape=(max_dec_seq_length, ))\n",
    "\n",
    "        encoder_embed_layer = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, input_length=max_enc_seq_length)\n",
    "        decoder_embed_layer = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, input_length=max_dec_seq_length)\n",
    "\n",
    "        encoder_embeded_inputs = encoder_embed_layer(encoder_inputs)\n",
    "        decoder_embeded_inputs = encoder_embed_layer(decoder_inputs)\n",
    "\n",
    "        encoder_layer = GRU(units=hidden_dim, return_sequences=True, return_state=True)\n",
    "        encoder_outputs, encoder_last_output = encoder_layer(encoder_embeded_inputs)\n",
    "        \n",
    "        pad_index, corrected_last_outputs = get_last_outputs(encoder_inputs, encoder_outputs, dimension=2, seq_length=max_enc_seq_length)\n",
    "        masked_encoder_outputs = Multiply()([encoder_outputs, pad_index])\n",
    "\n",
    "        decoder_layer = GRU(units=hidden_dim, return_sequences=True, return_state=False)\n",
    "        decoder_outputs = decoder_layer(decoder_embeded_inputs, initial_state=corrected_last_outputs)\n",
    "\n",
    "        # Attention\n",
    "        # 1. 각 decoder position에 대하여 전체 encoder 벡터 값 붙이기\n",
    "        reshaped_encoder_inputs = Reshape((max_enc_seq_length * hidden_dim, ))(masked_encoder_outputs)\n",
    "        reshaped_decoder_inputs = Reshape((max_dec_seq_length * hidden_dim, ))(decoder_outputs)\n",
    "        enc_repeat_vector = RepeatVector(max_dec_seq_length)(reshaped_encoder_inputs)\n",
    "        dec_repeat_vector = RepeatVector(max_enc_seq_length)(reshaped_decoder_inputs)\n",
    "        reshape_enc_repeat_vector = Reshape((max_dec_seq_length, max_enc_seq_length, hidden_dim))(enc_repeat_vector)\n",
    "        reshape_dec_repeat_vector = Reshape((max_enc_seq_length, max_dec_seq_length, hidden_dim))(dec_repeat_vector)\n",
    "        reshape_dec_repeat_vector = Permute((2, 1, 3))(reshape_dec_repeat_vector)\n",
    "\n",
    "        concat = Concatenate()([reshape_dec_repeat_vector, reshape_enc_repeat_vector])\n",
    "\n",
    "        # 2. 벡터들을 1차원짜리 스코어로 만들어주기\n",
    "        dense1_score = Dense(hidden_dim // 2, activation='tanh')(concat)\n",
    "        dense2_score = Dense(1)(dense1_score) # to make softmax comparison\n",
    "        dense2_score = Reshape((max_dec_seq_length, max_enc_seq_length))(dense2_score) # reshape to be 2 dims\n",
    "\n",
    "        softmax_score_layer = Softmax(axis=-1)\n",
    "        softmax_score = softmax_score_layer(dense2_score)\n",
    "\n",
    "        # 3. Score를 Encoder Outputs와 곱해주기 (여기서부터 체크하기)\n",
    "        reshaped_score = Reshape((max_dec_seq_length * max_enc_seq_length, ))(softmax_score)\n",
    "        score_repeat_vector = RepeatVector(hidden_dim)(reshaped_score)\n",
    "        reshape_score_repeat_vector = Reshape((hidden_dim, max_dec_seq_length, max_enc_seq_length))(score_repeat_vector)\n",
    "        repeat_score = Permute((2, 1, 3))(reshape_score_repeat_vector)\n",
    "\n",
    "        # (여기서부터 체크하기)\n",
    "        permute_e = Permute((2, 1))(masked_encoder_outputs)\n",
    "        reshaped_e = Reshape((hidden_dim * max_enc_seq_length, ))(permute_e)\n",
    "        repeat_e_vector = RepeatVector(max_dec_seq_length)(reshaped_e)\n",
    "        repeat_e = Reshape((max_dec_seq_length, hidden_dim, max_enc_seq_length))(repeat_e_vector)\n",
    "\n",
    "\n",
    "        attended_mat_layer = Multiply()\n",
    "        attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
    "\n",
    "        context_layer = Lambda(lambda x: K.sum(x, axis=-1))\n",
    "        context = context_layer(attended_mat)\n",
    "\n",
    "        concat_context_layer = Concatenate(axis=-1)\n",
    "        concat_context = concat_context_layer([context, decoder_outputs])\n",
    "\n",
    "        attention_output = Dense(hidden_dim, activation='tanh')(concat_context)\n",
    "\n",
    "\n",
    "        decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "        decoder_outputs_pred = decoder_dense(attention_output)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs_pred)\n",
    "    # Run training\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2648/2648 [==============================] - 1178s 445ms/step - loss: 2.3761 - val_loss: 3.6938\n",
      "Epoch 2/5\n",
      "2648/2648 [==============================] - 1178s 445ms/step - loss: 2.3462 - val_loss: 3.7690\n",
      "Epoch 3/5\n",
      "2648/2648 [==============================] - 1179s 445ms/step - loss: 2.2544 - val_loss: 3.8192\n",
      "Epoch 4/5\n",
      "2648/2648 [==============================] - 1179s 445ms/step - loss: 2.1987 - val_loss: 3.8775\n",
      "Epoch 5/5\n",
      "2648/2648 [==============================] - 1180s 445ms/step - loss: 2.1352 - val_loss: 3.9186\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model = get_seq2seq_luong_attention_model()\n",
    "train(model=seq2seq_model, train_data=(train_encoder_inputs, train_decoder_inputs, train_decoder_outputs), \n",
    "      validation_data=(test_encoder_inputs, test_decoder_inputs, test_decoder_outputs), epochs=5, batch_size=32, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-4. Attention Bahdanau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-5. Attention Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainwave_eeg",
   "language": "python",
   "name": "brainwave_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer의 사용법과 간략한 테스트 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword: Embedding, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 샘플 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명:\n",
    "## 10개 단어로 이루어진 문장을 index화 시킨 데이터라고 가정한다. \n",
    "## 가령 'I am the most handsome man in a lake town.' 이라는 문장이 있을 때, 마침표를 제외하면 10개짜리 단어가 된다. 단어에 대해 1대1로 매칭하는 유니크한 정수의 숫자로 변경하게 되면 [1, 5, 412, 337, 8, 158, 44, 887, 818, 9108] 라는 array로 표현할 수 있다.\n",
    "## 여기서 생성되는 샘플 데이터는 Sequence 길이가 10개인 어떤 index로 랜덤 생성하도록 한다. 이 데이터가 단어 10개짜리의 어떤 문장을 대변한다고 가정한다.\n",
    "## 단, 0는 패딩을 위한 숫자임으로 사용하지 않도록 주의한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 42 14 90 27 43 40 89 90 18]]\n",
      "Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 1\n",
    "sequence_length = 10\n",
    "max_count_word = 100\n",
    "sentences = np.random.randint(low=1, high=max_count_word, size=(num_sentence, sequence_length), dtype='int32')\n",
    "print(sentences)\n",
    "print('Shape:', sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명:\n",
    "## 위와 동일하지만, sequence 길이가 7개인 데이터를 생성한다. 즉, 7개의 단어로 이루어진 문장을 대변하는 샘플이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 60 79 65 47 31 73]]\n",
      "Shape: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 1\n",
    "sequence_length2 = 7\n",
    "max_count_word = 100\n",
    "sentences2 = np.random.randint(low=1, high=max_count_word, size=(num_sentence, sequence_length2), dtype='int32')\n",
    "print(sentences2)\n",
    "print('Shape:', sentences2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Embedding\n",
    "## 참조: https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명:\n",
    "## 단어와 1대1로 매칭하는 index 숫자로 바꾸어 학습을 시키게 되면, 단어와 단어간의 관계를 알 수 없어 모든 단어가 독립성을 유지하게 된다. 또한 해당 index 숫자는 학습의 결과가 반영되지 않는다. 해당 index 숫자는 학습을 할 때 변경되지 않기 때문이다. Embedding은 이러한 문제를 해결하고, 해당 단어를 더 많은 숫자로 대표하기 위해 벡터화 할 때 사용한다.\n",
    "## word2vec, glove, elmo 등 단어를 미리 n차원으로 미리 벡터화하여 활용할 수도 있지만, 그러한 과정보다는 빠르게 적용할 때에 용이하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_count_word, output_dim=embedding_dim, input_length=sequence_length))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.01682668  0.032153    0.03838123  0.03268946  0.04853031\n",
      "   -0.02516611  0.02209361 -0.00933857  0.00194546  0.03104175]\n",
      "  [ 0.01334042 -0.03259127  0.03382635  0.0163898   0.01688231\n",
      "   -0.04899883 -0.01359956  0.04509875  0.00611522  0.01252574]\n",
      "  [-0.00819968  0.0390888   0.03085455  0.00924366 -0.02313346\n",
      "   -0.03658485 -0.04107984 -0.00701517  0.01286194 -0.0379859 ]\n",
      "  [ 0.00242547  0.03038977 -0.02300781  0.01974389  0.00966261\n",
      "    0.04188225 -0.02840861 -0.01560032 -0.02577853  0.00842831]\n",
      "  [-0.02033303 -0.01707885 -0.03501876 -0.03323728 -0.01142213\n",
      "   -0.03994311 -0.04402068 -0.01533108 -0.00473094  0.03358055]\n",
      "  [-0.00120243  0.00408393  0.0445294  -0.04711903  0.03986431\n",
      "   -0.01524204  0.00226891  0.04626406 -0.03219088  0.02780307]\n",
      "  [-0.02486675  0.0126793   0.00273563  0.02066383  0.04796397\n",
      "   -0.00533837 -0.00161868  0.04458729  0.04686597  0.03406617]\n",
      "  [-0.0493327  -0.00536724 -0.04530083 -0.02869263  0.02285678\n",
      "   -0.00358162  0.03475207 -0.0388806  -0.00480864 -0.01806539]\n",
      "  [ 0.00242547  0.03038977 -0.02300781  0.01974389  0.00966261\n",
      "    0.04188225 -0.02840861 -0.01560032 -0.02577853  0.00842831]\n",
      "  [-0.03301313 -0.02921884 -0.03942053 -0.00668358 -0.0127034\n",
      "    0.0294475   0.04234176  0.00333836  0.00353336 -0.01056587]]]\n"
     ]
    }
   ],
   "source": [
    "embeded = model.predict(sentences)\n",
    "print(embeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(embeded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((embeded < -1) & (embeded > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코멘트: embedding 레이어의 초기값은 -1과 1사이에 존재한다. 다만 학습이 이루어짐에 따라 min값과 max값이 변경하는지 살펴볼 필요는 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Padding과 Embedding\n",
    "## 참조: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명:\n",
    "## 학습을 위해서는 각기 다른 sequence length를 갖는 데이터를 일정한 길이로 줄이거나 늘려야 한다. 줄이는 것은 잘라냄으로써 활용할 수 있지만, 그보다 작은 sequence length를 갖는 것은 padding을 통해 해결할 수 있다. Padding은 일반적으로 0로 활용한다. 위에서 단어와 1대1로 매칭하는 숫자에서 0를 사용하지 말라는 이유가 바로 이것 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: [[12 60 79 65 47 31 73]]\n",
      "Padded Sentence: [[12 60 79 65 47 31 73  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentence = pad_sequences(sentences2, maxlen=10, padding='post')\n",
    "print('Original Sentence:', sentences2)\n",
    "print('Padded Sentence:', padded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_count_word, output_dim=embedding_dim, input_length=sequence_length))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.9345516e-02 -1.6400885e-02  1.5126061e-02 -1.9885624e-02\n",
      "    3.4565341e-02 -2.8023768e-02  4.6606351e-02  1.9300092e-02\n",
      "    2.6570786e-02 -3.2560371e-02]\n",
      "  [-1.1222161e-02  4.5985851e-02 -4.1000377e-02 -3.4073163e-02\n",
      "    1.3448391e-02  9.0518221e-03  2.9488493e-02  1.8113006e-02\n",
      "    3.7224520e-02  3.2859873e-02]\n",
      "  [ 3.1362176e-03  4.4812750e-02 -4.2194914e-02  2.7369130e-02\n",
      "   -3.0939206e-03  6.1077848e-03  3.1876136e-02  4.3125283e-02\n",
      "   -2.8571248e-02  1.5073422e-02]\n",
      "  [-2.1986533e-02  4.9500372e-02 -3.9732672e-02 -1.8177330e-02\n",
      "   -1.2057614e-02  1.4890280e-02 -1.5734173e-02  1.0159146e-02\n",
      "   -4.3353714e-02  1.4022935e-02]\n",
      "  [-1.2183953e-02  1.9569065e-02  4.6443153e-02 -6.0424209e-06\n",
      "   -1.1860561e-02  3.3384923e-02 -1.5521646e-02 -4.1691102e-02\n",
      "   -3.0919528e-02  1.9085694e-02]\n",
      "  [ 4.8229646e-02 -2.2857083e-02  3.0424222e-03  4.4638600e-02\n",
      "   -4.7237527e-02  3.2914069e-02  1.0157205e-02  2.5621057e-03\n",
      "    8.8334084e-05 -4.9143482e-02]\n",
      "  [ 4.0760089e-02 -3.6812950e-02  4.4088807e-02 -3.2756343e-02\n",
      "    6.1804056e-04  2.0708714e-02  2.9933784e-02 -1.7947815e-02\n",
      "    2.9169384e-02 -8.3929412e-03]\n",
      "  [ 8.5268170e-04  9.2649087e-03  1.7529871e-02  1.5503753e-02\n",
      "    4.6971951e-02 -3.4625269e-02 -2.1286830e-03 -4.4449091e-02\n",
      "   -3.3349313e-02 -3.4238234e-02]\n",
      "  [ 8.5268170e-04  9.2649087e-03  1.7529871e-02  1.5503753e-02\n",
      "    4.6971951e-02 -3.4625269e-02 -2.1286830e-03 -4.4449091e-02\n",
      "   -3.3349313e-02 -3.4238234e-02]\n",
      "  [ 8.5268170e-04  9.2649087e-03  1.7529871e-02  1.5503753e-02\n",
      "    4.6971951e-02 -3.4625269e-02 -2.1286830e-03 -4.4449091e-02\n",
      "   -3.3349313e-02 -3.4238234e-02]]]\n"
     ]
    }
   ],
   "source": [
    "embeded = model.predict(padded_sentence)\n",
    "print(embeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embeded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((embeded < -1) & (embeded > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코멘트: 0에 해당하는 값도 embedding_dim에 해당하는 차원수의 데이터로 변경되는 것을 볼 수 있으며, 마지막 세 개의 embedding 값이 동일한 것도 확인할 수 있다. (0 패딩된 값 3개가 embedding 값으로 치환되었을 때 같은 값으로 나온다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainwave_eeg",
   "language": "python",
   "name": "brainwave_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

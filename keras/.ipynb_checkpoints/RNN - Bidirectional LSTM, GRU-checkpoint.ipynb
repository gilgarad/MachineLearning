{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, GRU, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 샘플 데이터 생성\n",
    "## 참조: http://121.140.2.142:8888/notebooks/test_notes/keras/Embedding%20Layer%20%26%20Padding.ipynb 의 샘플 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명: Sequence length 10짜리 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70 55 83 61 92 27 84  1 52 65]]\n",
      "Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 1\n",
    "sequence_length = 10\n",
    "max_count_word = 100\n",
    "sentences = np.random.randint(low=1, high=max_count_word, size=(num_sentence, sequence_length), dtype='int32')\n",
    "print(sentences)\n",
    "print('Shape:', sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명: Sequence length 7짜리 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  8 43  5 53 28 49]]\n",
      "Shape: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 1\n",
    "sequence_length2 = 7\n",
    "max_count_word = 100\n",
    "sentences2 = np.random.randint(low=1, high=max_count_word, size=(num_sentence, sequence_length2), dtype='int32')\n",
    "print(sentences2)\n",
    "print('Shape:', sentences2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GRU / LSTM bidirectional\n",
    "## (GRU / LSTM) http://121.140.2.142:8888/notebooks/test_notes/keras/RNN%20-%20LSTM%2C%20GRU.ipynb\n",
    "## 참조: https://keras.io/layers/recurrent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gru_model(hidden_dim, return_sequences, return_state, embedding):\n",
    "    model = Sequential()\n",
    "    if embedding:\n",
    "        model.add(Embedding(input_dim=100, output_dim=1, input_length=10))\n",
    "    model.add(Bidirectional(GRU(units=hidden_dim, return_sequences=return_sequences, return_state=return_state)))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    return model\n",
    "\n",
    "def get_lstm_model(hidden_dim, return_sequences, return_state, embedding):\n",
    "    model = Sequential()\n",
    "    if embedding:\n",
    "        model.add(Embedding(input_dim=100, output_dim=1, input_length=10))\n",
    "    model.add(Bidirectional(LSTM(units=hidden_dim, return_sequences=return_sequences, return_state=return_state)))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, model, sentences, embedding):\n",
    "    print('#### %s Model Test with embedding=%s ####' %(name, str(embedding)))\n",
    "    if not embedding: # if embedding not used, 2d -> 3d\n",
    "        _sentences = list()\n",
    "        for sentence in sentences:\n",
    "            _sentence = list()\n",
    "            for s in sentence:\n",
    "                _sentence.append([s])\n",
    "            _sentences.append(_sentence)\n",
    "    else:\n",
    "        _sentences = sentences\n",
    "\n",
    "    print('Change to correct format for RNN')\n",
    "    print('sentences -> _sentences')\n",
    "    print(sentences, '->', _sentences)\n",
    "    _sentences = np.array(_sentences) # need to be numpy array type ...\n",
    "    predicted = model.predict(_sentences)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 마지막 시퀀스 output만 가져옴 (default 파라미터 사용시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU와 LSTM 셀을 활용하며, embedding을 활용한 버전과 활용하지 않은 버전 두 가지를 적용하여 총 4개의 테스트 케이스가 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1\n",
    "return_sequences = False # Default\n",
    "return_state = False # Default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### GRU Model Test with embedding=False ####\n",
      "Change to correct format for RNN\n",
      "sentences -> _sentences\n",
      "[[70 55 83 61 92 27 84  1 52 65]] -> [[[70], [55], [83], [61], [92], [27], [84], [1], [52], [65]]]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMV launch failed:  m=1, n=1\n\t [[{{node bidirectional_1/while_1/MatMul_3}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while_1/Switch_3:1, bidirectional_1/while_1/MatMul_3/Enter)]]\n\t [[{{node bidirectional_1/concat/_35}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_384_bidirectional_1/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e37006c62ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                            return_state=return_state, embedding=embedding),\n\u001b[1;32m      8\u001b[0m                        \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                        \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                       )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-960e825c14f8>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(name, model, sentences, embedding)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0m_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to be numpy array type ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMV launch failed:  m=1, n=1\n\t [[{{node bidirectional_1/while_1/MatMul_3}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while_1/Switch_3:1, bidirectional_1/while_1/MatMul_3/Enter)]]\n\t [[{{node bidirectional_1/concat/_35}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_384_bidirectional_1/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = True\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = True\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코멘트: GRU와 LSTM 모두 last output만을 사용할 때에는 똑같은 형태의 input과 output 데이터를 가진다. 각각의 메카니즘 또는 embedding의 여부에 따라 output 값은 달라진다. embedding이 있고 없고에 따라 input data의 shape은 변경되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers import b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 모든 시퀀스 output을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1\n",
    "return_sequences = True # Changed\n",
    "return_state = False # Default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = True\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = True\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted)\n",
    "print('Shape:', predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코멘트: return_sequences 또한 GRU<->LSTM비교시 input과 output의 형태는 차이가 없다. 단지 embedding을 할 경우와 하지 않을 경우 input data의 shape은 달라진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 모든 시퀀스 output과 last output(+last hidden state)을 함께 가지고 옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1\n",
    "return_sequences = True # Changed\n",
    "return_state = True # Changed\n",
    "\n",
    "def get_gru_model2(hidden_dim, return_sequences, return_state, embedding):\n",
    "    \n",
    "    if embedding:\n",
    "        inputs = Input(shape=(10, ))\n",
    "        _inputs = Embedding(input_dim=100, output_dim=1, input_length=10)(inputs)\n",
    "    else:\n",
    "        inputs = Input(shape=(10, 1))\n",
    "        _inputs = inputs\n",
    "    outputs, last_output = GRU(units=hidden_dim, return_sequences=return_sequences, return_state=return_state)(_inputs)\n",
    "    outputs = [outputs, last_output]\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    return model\n",
    "\n",
    "def get_lstm_model2(hidden_dim, return_sequences, return_state, embedding):\n",
    "    if embedding:\n",
    "        inputs = Input(shape=(10, ))\n",
    "        _inputs = Embedding(input_dim=100, output_dim=1, input_length=10)(inputs)\n",
    "    else:\n",
    "        inputs = Input(shape=(10, 1))\n",
    "        _inputs = inputs\n",
    "    outputs, state_h, state_c = LSTM(units=hidden_dim, return_sequences=return_sequences, return_state=return_state)(_inputs)\n",
    "    outputs = [outputs, state_h, state_c]\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model2(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('GRU Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: GRU / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = True\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model2(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('GRU Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = False\n",
    "\"\"\"\n",
    "embedding = False\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model2(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('LSTM Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "print('')\n",
    "print('LSTM Last hidden_state by return_state:')\n",
    "print(predicted[2])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)\n",
    "print('Last Hidden State Shape:', predicted[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: LSTM / Embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = False\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model2(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('LSTM Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "print('')\n",
    "print('LSTM Last hidden_state by return_state:')\n",
    "print(predicted[2])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)\n",
    "print('Last Hidden State Shape:', predicted[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코멘트: Embedding의 유무는 output의 형태와 아무런 관련이 없다. GRU와 LSTM은 return_state와 return_sequences를 했을 때 결과의 갯수가 다르다. GRU는 last output만 반환하고 LSTM은 last cell state 또한 반환하기 때문이다. 이것은 두 셀 구조를 들여다 봄으로써 이해하길 권장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Return sequences=False, return state=True일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 레이어의 존재 유무는 지금부터는 default로 True로 놓고 볼 예정이다. 위에서 output shape은 동일함을 몇 번이나 확인했기 때문이다. 또한 Embedding 값이 True일 때 더 풍부한 값의 변화를 볼 수 있다. 이번에는 GRU와 LSTM의 return_sequences=False, return_state=True 상태일 때 나오는 값을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1\n",
    "return_sequences = False # Default\n",
    "return_state = True # Changed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: GRU / Embedding = False\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='GRU', \n",
    "                       model=get_gru_model2(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('GRU Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('GRU Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Case: LSTM / Embedding = False\n",
    "\"\"\"\n",
    "embedding = True\n",
    "predicted = test_model(name='LSTM', \n",
    "                       model=get_lstm_model3(hidden_dim=hidden_dim, return_sequences=return_sequences, \n",
    "                                           return_state=return_state, embedding=embedding),\n",
    "                       sentences=sentences,\n",
    "                       embedding=embedding\n",
    "                      )\n",
    "\n",
    "print('')\n",
    "print('LSTM Output:')\n",
    "print(predicted[0])\n",
    "print('')\n",
    "print('LSTM Last Output by return_state:')\n",
    "print(predicted[1])\n",
    "print('')\n",
    "print('LSTM Last hidden_state by return_state:')\n",
    "print(predicted[2])\n",
    "\n",
    "print('')\n",
    "print('Sequence Output Shape:', predicted[0].shape)\n",
    "print('Last Output Shape:', predicted[1].shape)\n",
    "print('Last Hidden State Shape:', predicted[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainwave_eeg",
   "language": "python",
   "name": "brainwave_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
